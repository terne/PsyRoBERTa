{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import azure-core elements\n",
        "import azureml.core\n",
        "from azureml.core.workspace import Workspace\n",
        "from azureml.core import ScriptRunConfig, Environment, Experiment\n",
        "from azureml.core.environment import CondaDependencies\n",
        "from azureml.core import Workspace, Datastore, Dataset\n",
        "from azureml.data.dataset_factory import DataType\n",
        "\n",
        "# Initiate workspace\n",
        "workspace = Workspace.from_config()\n",
        "\n",
        "# Define datastore and load dataset\n",
        "datastore_name = 'sp_data'\n",
        "datastore = Datastore.get(workspace, datastore_name)\n",
        "\n",
        "datastore_paths = [(datastore, '/patients.parquet')] \n",
        "ds = Dataset.Tabular.from_parquet_files(path=datastore_paths)\n",
        "dfP = ds.to_pandas_dataframe()\n",
        "dfP.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import Dataset, DatasetDict\n",
        "sns.set(style=\"whitegrid\")\n",
        "sns.set_palette('mako_r')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "events = pd.read_parquet(\"../../data/acuteReadmission/events_acute_labels.parquet\")\n",
        "# make new encounter column, as acute and non acute have different encounter columns in events dataframe\n",
        "events[\"encounter\"] = [int(i) if math.isnan(i)==False else int(j) for (i,j) in list(zip(events.EncounterKey_dis.values, events.EncounterKey.values))]\n",
        "\n",
        "event = events.merge(\n",
        "        dfP[[\"DurableKey\", \"BirthDate\"]],\n",
        "        left_on = 'PatientDurableKey',\n",
        "        right_on = 'DurableKey')\\\n",
        "        .drop(columns='DurableKey')\n",
        "event['Age'] = np.floor((pd.to_datetime(event.Date_dis) -pd.to_datetime(event.BirthDate)).dt.days / 365.25).astype(int)\n",
        "\n",
        "age_df = event[[\"encounter\", \"Age\"]].copy()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"../../finetuning/acutereadm_finetuned_models/dischargesum/\"\n",
        "model_name = \"psyroberta_p4_epoch12\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = AutoModel.from_pretrained(model_path+model_name, \n",
        "                                    local_files_only=True,\n",
        "                                    use_safetensors=True, \n",
        "                                    output_hidden_states=True,\n",
        "                                    output_attentions=True)\n",
        "model.to(device)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path+model_name, local_files_only=True)\n",
        "\n",
        "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import MLClient#, Input, command\n",
        "from azure.identity import DefaultAzureCredential\n",
        "import azure_ml_configs\n",
        "\n",
        "workspace_id = azure_ml_configs.workspace_id \n",
        "subscription_id = azure_ml_configs.subscription_id \n",
        "resource_group = azure_ml_configs.resource_group\n",
        "workspace_name = azure_ml_configs.workspace_name\n",
        "\n",
        "# Get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    credential=DefaultAzureCredential(),\n",
        "    subscription_id=subscription_id,\n",
        "    resource_group_name=resource_group,\n",
        "    workspace_name=workspace_name,\n",
        ")\n",
        "\n",
        "discharge_notes_only = True\n",
        "text_column_name = \"text_names_removed_step2\"\n",
        "\n",
        "data_asset = ml_client.data.get(name=\"clinicalNote_AcuteReadmission\", version=1)\n",
        "\n",
        "print(f\"Data asset URI: {data_asset.path}\")\n",
        "\n",
        "data_path = data_asset.path"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# loading and prepraring data\n",
        "cols = [text_column_name, \"Acute\", \"set\", \"Type\", \"PatientDurableKey\", \"EncounterKey\", \"CreationInstant\"]\n",
        "df = pd.read_csv(data_path, usecols=cols)\n",
        "# make sure the data is sorted by patient id, encounter and date\n",
        "df.sort_values(by=[\"PatientDurableKey\", \"EncounterKey\", \"CreationInstant\"],inplace=True)\n",
        "#rename main columns of interest\n",
        "df.rename(columns={text_column_name: \"text\", \"Acute\": \"label\"}, inplace=True)\n",
        "\n",
        "if discharge_notes_only:\n",
        "    df = df[df[\"Type\"].str.contains(\"Udskrivningsresume|Udskrivningsresum√©\")==True].copy()\n",
        "    \n",
        "\n",
        "# concatenating texts on patient and encounter id\n",
        "df = df.groupby([\"PatientDurableKey\", \"EncounterKey\", \"label\", \"set\"]).text.apply(f'{tokenizer.sep_token}'.join).reset_index()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing text\n",
        "\n",
        "data_dict = {\n",
        "    \"train\": Dataset.from_pandas(df[df.set==\"train\"]),\n",
        "    \"validation\": Dataset.from_pandas(df[df.set==\"val\"]),\n",
        "    \"test\": Dataset.from_pandas(df[df.set==\"test\"])\n",
        "    }\n",
        "\n",
        "\n",
        "raw_datasets = DatasetDict(data_dict)\n",
        "\n",
        "text_column_name = \"text\"\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    labs = []\n",
        "    patientids = []\n",
        "    encounterids = []\n",
        "    texts = []\n",
        "    for x,y, patient_id, encounter_id in list(zip(examples[\"text\"], examples[\"label\"], examples[\"PatientDurableKey\"], examples[\"EncounterKey\"])):\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            x,  # Sentence to encode\n",
        "            add_special_tokens=True,  # Add '[CLS]' and '[SEP]' or equivelant for roberta\n",
        "            max_length=512,  # Pad & truncate all sentences.\n",
        "            padding=\"max_length\", #(needing to specify truncation=True depends on version)\n",
        "            truncation=True,\n",
        "            return_overflowing_tokens=True, # return lists of tokens above 512 \n",
        "            return_offsets_mapping=True,\n",
        "            stride=32, # The stride used when the context is too large and is split across several features.\n",
        "            return_attention_mask=True,  # Construct attn. masks.\n",
        "            return_tensors='pt'  # Return pytorch tensors.\n",
        "        )\n",
        "        for inputs, attentions in list(zip(encoded_dict['input_ids'],encoded_dict['attention_mask']))[:None]:\n",
        "            #print(i.shape)\n",
        "            # Add the encoded sentence to the list.\n",
        "            input_ids.append(inputs)\n",
        "            texts.append(tokenizer.decode(inputs))\n",
        "            #And its attention mask (simply differentiates padding from non-padding).\n",
        "            attention_masks.append(attentions)\n",
        "            labs.append(y)\n",
        "            patientids.append(patient_id)\n",
        "            encounterids.append(encounter_id)\n",
        "    assert len(input_ids) == len(attention_masks) == len(labs) == len(patientids) == len(encounterids)\n",
        "    sample = {\"inputs\": input_ids,\n",
        "            \"attn_masks\": attention_masks,\n",
        "            \"labels\": labs,\n",
        "            \"patient_id\": patientids,\n",
        "            \"encounter_id\": encounterids,\n",
        "            \"text_split\":texts}\n",
        "    return sample\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "            tokenize_function,\n",
        "            batched=True,\n",
        "            num_proc=None,\n",
        "            remove_columns=raw_datasets['validation'].column_names,\n",
        "            #load_from_cache_file=not args.overwrite_cache,\n",
        "            desc=\"Running tokenizer on every text in dataset\",\n",
        "        )\n",
        "\n",
        "tokenized_datasets[\"train\"].set_format(type='pt', columns=['inputs', 'attn_masks', 'labels', 'patient_id', 'encounter_id'])\n",
        "tokenized_datasets[\"validation\"].set_format(type='pt', columns=['inputs', 'attn_masks', 'labels', 'patient_id', 'encounter_id'])\n",
        "#if args.on_test:\n",
        "tokenized_datasets[\"test\"].set_format(type='pt',columns=['inputs', 'attn_masks', 'labels', 'patient_id', 'encounter_id'])\n",
        "\n",
        "traindata = tokenized_datasets[\"train\"]\n",
        "valdata = tokenized_datasets[\"validation\"]\n",
        "#if args.on_test:\n",
        "testdata = tokenized_datasets[\"test\"]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# load saved embeddings (generated in embeddings_analyses_PCA_KNN.ipynb)\n",
        "\n",
        "train_cls_embeddings = np.load(\"train_cls_embeddings.npy\")\n",
        "train_mean_pooled_embeddings = np.load(\"train_mean_pooled_embeddings.npy\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def agegroups(x):\n",
        "    if x==\"Unknown\":\n",
        "        return x\n",
        "    elif x < 18:\n",
        "        return \"Below 18\"\n",
        "    elif x >= 18 and x < 35:\n",
        "        return \"18-34\"\n",
        "    elif x >= 35 and x < 55:\n",
        "        return \"35-54\"\n",
        "    else:\n",
        "        return \"Above 54\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "preds = pd.read_csv(\"../../result_files/dischargesum_psyroberta_p4_epoch12_AR_train_results.csv\", usecols=[\"pos_prob\", \"pid\", \"eid\"])\n",
        "preds = preds.groupby(by=[\"eid\",\"pid\"])[\"pos_prob\"].mean().reset_index()\n",
        "preds[\"pred\"] = preds.pos_prob.apply(lambda x: 0 if x<0.5 else 1)\n",
        "\n",
        "train_labels = traindata[\"labels\"]\n",
        "train_eids = traindata[\"encounter_id\"]\n",
        "train_pids = traindata[\"patient_id\"]\n",
        "train_encounter_embs_cls = np.array([np.array([train_cls_embeddings[i] for i in np.where(np.array(train_eids) == k)[0]]).mean(axis=0) for k in np.unique(train_eids)])\n",
        "#train_num_notes_in_encounter = [len([train_cls_embeddings[i] for i in np.where(np.array(train_eids) == k)[0]]) for k in np.unique(train_eids)]\n",
        "train_encounter_embs_mean_pooled = np.array([np.array([train_mean_pooled_embeddings[i] for i in np.where(np.array(train_eids) == k)[0]]).mean(axis=0) for k in np.unique(train_eids)])\n",
        "train_encounter_labs = np.array([np.array([train_labels[i] for i in np.where(np.array(train_eids) == k)[0]]).mean(axis=0) for k in np.unique(train_eids)])\n",
        "train_num_notes_in_encounter = np.array([len([train_labels[i] for i in np.where(np.array(train_eids) == k)[0]]) for k in np.unique(train_eids)])\n",
        "train_encounter_pid = np.array([[train_pids[i].item() for i in np.where(np.array(train_eids) == k)[0]][0] for k in np.unique(train_eids)])\n",
        "\n",
        "train_probs = np.array([preds[preds.eid==i].pos_prob.item() for i in np.unique(train_eids)])\n",
        "train_preds = np.array([preds[preds.eid==i].pred.item() for i in np.unique(train_eids)])\n",
        "#train_encounter_tokens =  []\n",
        "\n",
        "intersection_train = set(train_encounter_pid.tolist()).intersection(set(dfP.DurableKey.values.tolist()))\n",
        "train_sex = [dfP[dfP[\"DurableKey\"]==i].Sex.item() if i in intersection_train else \"Unknown\" for i in train_encounter_pid]\n",
        "train_eth = [dfP[dfP[\"DurableKey\"]==i].Ethnicity.item() if i in intersection_train else \"Unknown\" for i in train_encounter_pid]\n",
        "train_age = [age_df[age_df.encounter==i].Age.item() if i in age_df.encounter.values else \"Unknown\" for i in np.unique(train_eids)]\n",
        "train_age_groups = [agegroups(i) for i in train_age]\n",
        "\n",
        "print(train_encounter_embs_cls.shape)\n",
        "print(train_encounter_embs_mean_pooled.shape)\n",
        "print(train_encounter_labs.shape)\n",
        "print(train_num_notes_in_encounter.shape)\n",
        "print(train_encounter_pid.shape)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_diagnosis = np.load(\"train_diagnosis.npy\")\n",
        "\n",
        "def skscode_to_diagnosis(sks):\n",
        "    if sks.startswith(\"DF20\"):\n",
        "        return \"Schizophrenia\"\n",
        "    elif sks.startswith(\"DF2\"):\n",
        "        return \"Other psychosis\"\n",
        "    elif sks.startswith(\"DF30\") or sks.startswith(\"DF31\"):\n",
        "        return \"Bipolar/manic\"\n",
        "    elif sks.startswith(\"DF32\") or sks.startswith(\"DF33\"):\n",
        "        return \"Depression\"\n",
        "    elif sks.startswith(\"DF40\") or sks.startswith(\"DF41\") or sks.startswith(\"DF42\"):\n",
        "        return \"Anxiety/OCD\"\n",
        "    elif sks.startswith(\"DF6\"):\n",
        "        return \"Personality disorder\"\n",
        "    elif sks.startswith(\"DF1\"):\n",
        "        return \"SUD\"\n",
        "    else:\n",
        "        return \"Other\"\n",
        "\n",
        "train_diagnosis_simple = np.array([sks[1:3] if sks.startswith(\"DF\") else \"Other\" for sks in train_diagnosis])\n",
        "\n",
        "train_diagnosis_specific = np.array([skscode_to_diagnosis(sks) for sks in train_diagnosis])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "neighbors = [5, 20, 80, 320]\n",
        "min_dists = [0.0125, 0.05, 0.2, 0.8]\n",
        "metrics = [\"cosine\", \"euclidean\"]\n",
        "\n",
        "for k in neighbors:\n",
        "    for j in min_dists:\n",
        "        for m in metrics:\n",
        "            reducer = umap.UMAP(n_neighbors=k, min_dist=j, metric=m)\n",
        "            #scaled_mean_pooled = StandardScaler().fit_transform(train_encounter_embs_mean_pooled)\n",
        "            umap_mean_pooled = reducer.fit_transform(train_encounter_embs_mean_pooled)\n",
        "\n",
        "            plot_data_mean_pooled = {'UMAP 1': umap_mean_pooled[:, 0], \n",
        "                                    'UMAP 2': umap_mean_pooled[:, 1],\n",
        "                                    'Label': [int(i) for i in train_encounter_labs],\n",
        "                                    'Num notes': train_num_notes_in_encounter,\n",
        "                                    \"Sex\": train_sex,\n",
        "                                    \"Ethnicity\": train_eth,\n",
        "                                    \"Age\": train_age_groups,\n",
        "                                    \"Prediction\": train_preds,\n",
        "                                    \"Probability\": train_probs,\n",
        "                                    \"Diagnosis\": train_diagnosis_simple,\n",
        "                                    \"Diagnosis_specific\": train_diagnosis_specific\n",
        "                                    }\n",
        "            with open('umap_train_k{}_mindist{}_{}.pkl'.format(k,j,m), 'wb') as f:\n",
        "                pickle.dump(plot_data_mean_pooled, f)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_style(\"white\", {\"axes.edgecolor\": \".8\"})\n",
        "\n",
        "c0 = sns.color_palette(palette='mako_r',n_colors=6)[0]\n",
        "c1 = sns.color_palette(palette='mako_r',n_colors=6)[4]\n",
        "\n",
        "colors=[c0,c1]\n",
        "\n",
        "fig, ax = plt.subplots(4,4, figsize=(10,10))\n",
        "\n",
        "neighbors = [5, 20, 80, 320]\n",
        "min_dists = [0.0125, 0.05, 0.2, 0.8]\n",
        "metric = \"euclidean\"\n",
        "\n",
        "for i,k in enumerate(neighbors):\n",
        "    for j,m_dist in enumerate(min_dists):\n",
        "        ax[j,i].set_xticks([]);\n",
        "        ax[j,i].set_yticks([]);\n",
        "        ax[0,i].set_title(\"n_neighbors={}\".format(k), fontsize=9);\n",
        "        ax[j,0].set_ylabel(\"min_dist={}\".format(m_dist), fontsize=9);\n",
        "        \n",
        "        \n",
        "        with open('umap_train_k{}_mindist{}_{}.pkl'.format(k,m_dist,metric), 'rb') as f:\n",
        "            plot_data_mean_pooled = pickle.load(f)\n",
        "            \n",
        "            sns.scatterplot(data=plot_data_mean_pooled, \n",
        "                            x='UMAP 1', \n",
        "                            y='UMAP 2',\n",
        "                            hue='Label', \n",
        "                            s=0.5, \n",
        "                            palette=colors, \n",
        "                            ax=ax[j,i])\n",
        "            ax[j,i].legend([],[], frameon=False)\n",
        "handles, labels = plt.gca().get_legend_handles_labels()\n",
        "by_label = dict(zip(labels, handles))\n",
        "fig.legend(by_label.values(), by_label.keys(), loc=(0.42, 0.92), title=\"Label\", ncol=2)\n",
        "plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
        "        \n",
        "plt.savefig(\"../../output/UMAP_train_{}_label.pdf\".format(metric), bbox_inches=\"tight\")\n",
        "plt.savefig(\"../../output/UMAP_train_{}_label.png\".format(metric), bbox_inches=\"tight\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_style(\"white\", {\"axes.edgecolor\": \".8\"})\n",
        "\n",
        "c0 = sns.color_palette(palette='mako_r',n_colors=6)[0]\n",
        "c1 = sns.color_palette(palette='mako_r',n_colors=6)[4]\n",
        "\n",
        "colors=[c0,c1]\n",
        "\n",
        "fig, ax = plt.subplots(4,4, figsize=(10,10))\n",
        "\n",
        "neighbors = [5, 20, 80, 320]\n",
        "min_dists = [0.0125, 0.05, 0.2, 0.8]\n",
        "metric = \"euclidean\"\n",
        "\n",
        "for i,k in enumerate(neighbors):\n",
        "    for j,m_dist in enumerate(min_dists):\n",
        "        ax[j,i].set_xticks([]);\n",
        "        ax[j,i].set_yticks([]);\n",
        "        ax[0,i].set_title(\"n_neighbors={}\".format(k), fontsize=9);\n",
        "        ax[j,0].set_ylabel(\"min_dist={}\".format(m_dist), fontsize=9);\n",
        "        \n",
        "        \n",
        "        with open('umap_train_k{}_mindist{}_{}.pkl'.format(k,m_dist,metric), 'rb') as f:\n",
        "            plot_data_mean_pooled = pickle.load(f)\n",
        "            \n",
        "            sns.scatterplot(data=plot_data_mean_pooled, \n",
        "                            x='UMAP 1', \n",
        "                            y='UMAP 2',\n",
        "                            hue='Prediction', \n",
        "                            s=0.5, \n",
        "                            palette=colors, \n",
        "                            ax=ax[j,i])\n",
        "            ax[j,i].legend([],[], frameon=False)\n",
        "handles, labels = plt.gca().get_legend_handles_labels()\n",
        "by_label = dict(zip(labels, handles))\n",
        "fig.legend(by_label.values(), by_label.keys(), loc=(0.42, 0.92), title=\"Prediction\", ncol=2)\n",
        "plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
        "        \n",
        "plt.savefig(\"../../output/UMAP_train_{}_prediction.pdf\".format(metric), bbox_inches=\"tight\")\n",
        "plt.savefig(\"../../output/UMAP_train_{}_prediction.png\".format(metric), bbox_inches=\"tight\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_style(\"white\", {\"axes.edgecolor\": \".8\"})\n",
        "\n",
        "colors = sns.color_palette(palette='colorblind',n_colors=11)\n",
        "colors = [colors[0]]+colors[2:5]+colors[7:10]\n",
        "\n",
        "fig, ax = plt.subplots(4,4, figsize=(10,10))\n",
        "\n",
        "neighbors = [5, 20, 80, 320]\n",
        "min_dists = [0.0125, 0.05, 0.2, 0.8]\n",
        "metric = \"cosine\"\n",
        "\n",
        "for i,k in enumerate(neighbors):\n",
        "    for j,m_dist in enumerate(min_dists):\n",
        "        ax[j,i].set_xticks([]);\n",
        "        ax[j,i].set_yticks([]);\n",
        "        ax[0,i].set_title(\"n_neighbors={}\".format(k), fontsize=9);\n",
        "        ax[j,0].set_ylabel(\"min_dist={}\".format(m_dist), fontsize=9);\n",
        "        \n",
        "        \n",
        "        with open('umap_train_k{}_mindist{}_{}.pkl'.format(k,m_dist,metric), 'rb') as f:\n",
        "            plot_data_mean_pooled = pickle.load(f)\n",
        "            \n",
        "            sns.scatterplot(data=plot_data_mean_pooled, \n",
        "                            x='UMAP 1', \n",
        "                            y='UMAP 2',\n",
        "                            hue='Diagnosis_specific', \n",
        "                            palette=colors+[\"black\"], \n",
        "                            #style=\"Diagnosis_specific\",\n",
        "                            hue_order= ['Anxiety/OCD', \n",
        "                                        'Personality disorder', \n",
        "                                        'Bipolar/manic', \n",
        "                                        'Depression', \n",
        "                                        'Other psychosis', \n",
        "                                        'SUD', \n",
        "                                        'Schizophrenia' \n",
        "                                        ][::-1]+[\"Other\"], \n",
        "                            s=0.5, \n",
        "                            ax=ax[j,i])\n",
        "            \n",
        "            ax[j,i].legend([],[], frameon=False)\n",
        "handles, labels = plt.gca().get_legend_handles_labels()\n",
        "by_label = dict(zip(labels, handles))\n",
        "fig.legend(by_label.values(), by_label.keys(), loc=(0.1, 0.92), title=\"Diagnosis\", ncol=4)\n",
        "plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
        "        \n",
        "plt.savefig(\"../../output/UMAP_train_{}_diagnosis_specific.pdf\".format(metric), bbox_inches=\"tight\")\n",
        "plt.savefig(\"../../output/UMAP_train_{}_diagnosis_specific.png\".format(metric), bbox_inches=\"tight\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}