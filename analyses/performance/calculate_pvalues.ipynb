{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "import sys\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score\n",
        "sys.path.append('../..')\n",
        "from utils.eval_utils import get_temp_df, compute_mcc \n",
        "np.random.seed(12)\n",
        "\n",
        "sns.set_palette('mako_r')\n",
        "sns.set_style(\"white\", {\"axes.edgecolor\": \".8\"})"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" \n",
        "\n",
        "The functions in this cell are from https://gist.github.com/rajpurkar/f96c131ba3aeffb1927255d4363496a9 \n",
        "Please see code description in their repo.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def calculate_metric(metric, model_predictions, test_labels):\n",
        "    # edited this function to include mcc\n",
        "    if metric == 'auc':\n",
        "        return roc_auc_score(test_labels, model_predictions)\n",
        "    elif metric == 'mcc':\n",
        "        binary_predictions = (model_predictions >= 0.5).astype(int)\n",
        "        return compute_mcc(test_labels, binary_predictions)\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid metric: {metric}\")\n",
        "\n",
        "def print_model_results(model_name, metric, model_predictions, test_set_labels):\n",
        "    print(f\"{model_name}:\")\n",
        "    print(f\"{metric.capitalize()}: {calculate_metric(metric, model_predictions, test_set_labels)}\")\n",
        "    print()\n",
        "\n",
        "def calculate_bootstrap_difference(metric, model_1_predictions, model_2_predictions, test_set_labels, n_resamples=1000):\n",
        "    \"\"\"\n",
        "    Calculate the metric differences between two models using bootstrapping.\n",
        "    Parameters:\n",
        "        metric (str): The metric to calculate ('auc' or 'accuracy').\n",
        "        model_1_predictions (numpy.ndarray): Model 1 predictions.\n",
        "        model_2_predictions (numpy.ndarray): Model 2 predictions.\n",
        "        test_set_labels (numpy.ndarray): Binary test labels.\n",
        "        n_resamples (int): Number of resamples in bootstrapping.\n",
        "    Returns:\n",
        "        numpy.ndarray: Array of differences in the metric for each bootstrapped sample.\n",
        "        float: Observed difference in the metric on the original test set.\n",
        "    \"\"\"\n",
        "    model_1_metric = calculate_metric(metric, model_1_predictions, test_set_labels)\n",
        "    model_2_metric = calculate_metric(metric, model_2_predictions, test_set_labels)\n",
        "    observed_difference = model_1_metric - model_2_metric\n",
        "\n",
        "    differences = np.empty(n_resamples)\n",
        "    n_samples = len(test_set_labels)\n",
        "\n",
        "    for i in range(n_resamples):\n",
        "        bootstrap_indices = np.random.choice(range(n_samples), size=n_samples, replace=True)\n",
        "        new_test_set_labels = test_set_labels[bootstrap_indices]\n",
        "        new_model_1_predictions = model_1_predictions[bootstrap_indices]\n",
        "        new_model_2_predictions = model_2_predictions[bootstrap_indices]\n",
        "\n",
        "        model_1_metric = calculate_metric(metric, new_model_1_predictions, new_test_set_labels)\n",
        "        model_2_metric = calculate_metric(metric, new_model_2_predictions, new_test_set_labels)\n",
        "        differences[i] = model_1_metric - model_2_metric\n",
        "\n",
        "    differences = differences - observed_difference\n",
        "\n",
        "    return differences, observed_difference\n",
        "\n",
        "def calculate_p_value(differences, observed_difference):\n",
        "    return sum(np.abs(differences) >= np.abs(observed_difference)) / len(differences)\n",
        "\n",
        "def interpret_p_value(p_value):\n",
        "    null_hypothesis = \"There is no difference in the performance of the two models.\"\n",
        "    alternative_hypothesis = \"There is a difference in the performance of the two models.\"\n",
        "    if p_value < 0.05:\n",
        "        print(\"Reject the null hypothesis in favor of the alternative hypothesis.\")\n",
        "        print(f\"{alternative_hypothesis} (p-value = {p_value:e})\")\n",
        "    else:\n",
        "        print(\"Fail to reject the null hypothesis.\")\n",
        "        print(f\"{null_hypothesis} (p-value = {p_value:e})\")\n",
        "\n",
        "def plot_histogram(differences, observed_difference, metric):\n",
        "    plt.figure()\n",
        "    plt.hist(differences, bins='auto')\n",
        "    plt.axvline(observed_difference, color='r', linestyle='dashed', linewidth=2, label='Observed Difference')\n",
        "    plt.title(f'Histogram of Bootstrapped Differences ({metric.upper()})')\n",
        "    plt.xlabel(f'Difference in {metric.upper()}')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# loading result files\n",
        "\n",
        "dischargesum_psyroberta_p4_epoch12 = pd.read_csv(\"../../result_files/dischargesum_psyroberta_p4_epoch12_results.csv\")\n",
        "dischargesum_roberta_epoch12 = pd.read_csv(\"../../result_files/dischargesum_roberta_epoch12_results.csv\")\n",
        "\n",
        "allnotes_psyroberta_p4_epoch12 = pd.read_csv(\"../../result_files/allnotes_psyroberta_p4_epoch12_results.csv\")\n",
        "allnotes_psyroberta_p4_dedupcont_epoch12 = pd.read_csv(\"../../result_files/allnotes_psyroberta_p4_dedupcont_epoch12_results.csv\")\n",
        "allnotes_roberta_dedupcont_epoch12 = pd.read_csv(\"../../result_files/allnotes_roberta_dedupcont_epoch12_results.csv\")\n",
        "\n",
        "dischargesum_medabert = pd.read_csv(\"../../result_files/dischargesum_medabert_results.csv\")\n",
        "allnotes_medabert_dedupcont = pd.read_csv(\"../../result_files/allnotes_medabert_results.csv\")\n",
        "\n",
        "dischargesum_bert = pd.read_csv(\"../../result_files/dischargesum_bert_results.csv\")\n",
        "allnotes_bert_dedupcont = pd.read_csv(\"../../result_files/allnotes_bert_results.csv\")\n",
        "\n",
        "lr_results_path1 = \"../../logistic_regression/LogRegDischargeBest_temp_test.csv\"\n",
        "lr_dis_temp_test = pd.read_csv(lr_results_path1, index_col=0)\n",
        "\n",
        "lr_results_path2 = \"../../logistic_regression/LogRegAllNotDeDupBest_temp_test.csv\"\n",
        "lr_all_temp_test = pd.read_csv(lr_results_path2, index_col=0)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "models_discharge = [dischargesum_psyroberta_p4_epoch12, \n",
        "          dischargesum_roberta_epoch12,\n",
        "          dischargesum_medabert,\n",
        "          dischargesum_bert,\n",
        "          lr_dis_temp_test]\n",
        "\n",
        "models_allnotes = [allnotes_psyroberta_p4_dedupcont_epoch12,\n",
        "          allnotes_roberta_dedupcont_epoch12,\n",
        "          allnotes_medabert_dedupcont,\n",
        "          allnotes_bert_dedupcont,\n",
        "          lr_all_temp_test]\n",
        "\n",
        "model_names_discharge = [\"PsyRoBERTa (Discharge Summaries)\",\n",
        "               \"RøBÆRTa (Discharge Summaries)\",\n",
        "               \"MeDa-BERT (Discharge Summaries)\",\n",
        "               \"BERT (Discharge Summaries)\",\n",
        "               \"LR (Discharge Summaries)\"]\n",
        "\n",
        "model_names_allnotes = [\"PsyRoBERTa (All Notes)\",\n",
        "               \"RøBÆRTa (All Notes)\",\n",
        "               \"MeDa-BERT (All Notes)\",\n",
        "               \"BERT (All Notes)\",\n",
        "               \"LR (All Notes)\"]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCH=11\n",
        "\n",
        "arr1 = np.zeros(shape=(len(models_discharge), len(models_discharge)))\n",
        "\n",
        "for i, (m1, model_name1) in enumerate(list(zip(models_discharge, model_names_discharge))):\n",
        "\n",
        "    if \"LR\" in model_name1:\n",
        "        temp_test1 = m1\n",
        "    else:\n",
        "        res1 = m1[m1.epoch==EPOCH]\n",
        "        temp_test1 = get_temp_df(res1, split=\"test\")\n",
        "\n",
        "    for j, (m2, model_name2) in enumerate(list(zip(models_discharge, model_names_discharge))):\n",
        "        \n",
        "        if \"LR\" in model_name2:\n",
        "            temp_test2 = m2\n",
        "        else:\n",
        "            res2 = m2[m2.epoch==EPOCH]\n",
        "            temp_test2 = get_temp_df(res2, split=\"test\")\n",
        "\n",
        "        print(model_name1, \"VS\", model_name2)\n",
        "        print()\n",
        "\n",
        "        print_model_results(model_name1, \"mcc\", temp_test1.p_mean, temp_test1.target)\n",
        "        print_model_results(model_name2, \"mcc\", temp_test2.p_mean, temp_test2.target)\n",
        "        #print_model_results(model_name1, \"auc\", temp_test1.p_mean, temp_test1.target)\n",
        "        #print_model_results(model_name2, \"auc\", temp_test2.p_mean, temp_test2.target)\n",
        "\n",
        "        assert temp_test1.target.values.tolist()==temp_test2.target.values.tolist()\n",
        "\n",
        "        differences, observed_difference = calculate_bootstrap_difference(\"mcc\", temp_test1.p_mean, temp_test2.p_mean, temp_test1.target, n_resamples=1000)\n",
        "        #differences, observed_difference = calculate_bootstrap_difference(\"auc\", temp_test1.p_mean, temp_test2.p_mean, temp_test1.target, n_resamples=1000)\n",
        "\n",
        "        pval = calculate_p_value(differences, observed_difference)\n",
        "\n",
        "        arr1[i,j] = pval\n",
        "\n",
        "        print(pval)\n",
        "\n",
        "        interpret_p_value(pval)\n",
        "\n",
        "        plot_histogram(differences, observed_difference, \"mcc\")\n",
        "        #plot_histogram(differences, observed_difference, \"auc\")\n",
        "        \n",
        "        plt.savefig(f\"../../output/pval_bootstraphist_{model_name1}_VS_{model_name2}.png\", bbox_inches=\"tight\")\n",
        "        plt.savefig(f\"../../output/pval_bootstraphist_{model_name1}_VS_{model_name2}.pdf\", bbox_inches=\"tight\")\n",
        "        \n",
        "        #plt.savefig(f\"../../output/pval_bootstraphist_AUC_{model_name1}_VS_{model_name2}.png\", bbox_inches=\"tight\")\n",
        "        #plt.savefig(f\"../../output/pval_bootstraphist_AUC_{model_name1}_VS_{model_name2}.pdf\", bbox_inches=\"tight\")\n",
        "        \n",
        "        print(\"________________________________\")\n",
        "        print()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCH=11\n",
        "\n",
        "arr2 = np.zeros(shape=(len(models_allnotes), len(models_allnotes)))\n",
        "\n",
        "for i, (m1, model_name1) in enumerate(list(zip(models_allnotes, model_names_allnotes))):\n",
        "\n",
        "    if \"LR\" in model_name1:\n",
        "        temp_test1 = m1\n",
        "    else:\n",
        "        res1 = m1[m1.epoch==EPOCH]\n",
        "        temp_test1 = get_temp_df(res1, split=\"test\")\n",
        "\n",
        "    for j, (m2, model_name2) in enumerate(list(zip(models_allnotes, model_names_allnotes))):\n",
        "        \n",
        "        if \"LR\" in model_name2:\n",
        "            temp_test2 = m2\n",
        "        else:\n",
        "            res2 = m2[m2.epoch==EPOCH]\n",
        "            temp_test2 = get_temp_df(res2, split=\"test\")\n",
        "\n",
        "        print(model_name1, \"VS\", model_name2)\n",
        "        print()\n",
        "\n",
        "        print_model_results(model_name1, \"mcc\", temp_test1.p_mean, temp_test1.target)\n",
        "        print_model_results(model_name2, \"mcc\", temp_test2.p_mean, temp_test2.target)\n",
        "        #print_model_results(model_name1, \"auc\", temp_test1.p_mean, temp_test1.target)\n",
        "        #print_model_results(model_name2, \"auc\", temp_test2.p_mean, temp_test2.target)\n",
        "\n",
        "        assert temp_test1.target.values.tolist()==temp_test2.target.values.tolist()\n",
        "\n",
        "        differences, observed_difference = calculate_bootstrap_difference(\"mcc\", temp_test1.p_mean, temp_test2.p_mean, temp_test1.target, n_resamples=1000)\n",
        "        #differences, observed_difference = calculate_bootstrap_difference(\"auc\", temp_test1.p_mean, temp_test2.p_mean, temp_test1.target, n_resamples=1000)\n",
        "\n",
        "        pval = calculate_p_value(differences, observed_difference)\n",
        "\n",
        "        arr2[i,j] = pval\n",
        "\n",
        "        print(np.round(pval, 10))\n",
        "\n",
        "        interpret_p_value(pval)\n",
        "\n",
        "        plot_histogram(differences, observed_difference, \"mcc\")\n",
        "        #plot_histogram(differences, observed_difference, \"auc\")\n",
        "\n",
        "        plt.savefig(f\"../../output/pval_bootstraphist_AllNotes_{model_name1}_VS_{model_name2}.png\", bbox_inches=\"tight\")\n",
        "        plt.savefig(f\"../../output/pval_bootstraphist_AllNotes_{model_name1}_VS_{model_name2}.pdf\", bbox_inches=\"tight\")\n",
        "        \n",
        "        #plt.savefig(f\"../../output/pval_bootstraphist_AUC_AllNotes_{model_name1}_VS_{model_name2}.png\", bbox_inches=\"tight\")\n",
        "        #plt.savefig(f\"../../output/pval_bootstraphist_AUC_Allnotes_{model_name1}_VS_{model_name2}.pdf\", bbox_inches=\"tight\")\n",
        "        \n",
        "        print(\"________________________________\")\n",
        "        print()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"arr1_discharge_mcc_pval.pkl\", \"wb\") as file:\n",
        "    pickle.dump(arr1, file)\n",
        "\n",
        "#with open(\"arr1_discharge_auc_pval.pkl\", \"wb\") as file:\n",
        "#    pickle.dump(arr1, file)\n",
        "\n",
        "with open(\"arr2_allnotes_mcc_pval.pkl\", \"wb\") as file:\n",
        "    pickle.dump(arr2, file)\n",
        "\n",
        "#with open(\"arr2_allnotes_auc_pval.pkl\", \"wb\") as file:\n",
        "#    pickle.dump(arr2, file)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "from matplotlib.ticker import AutoMinorLocator\n",
        "\n",
        "# Functions from matplotlib docs\n",
        "\n",
        "def heatmap(data, row_labels, col_labels, ax=None,\n",
        "            cbar_kw=None, cbarlabel=\"\", mask=True, **kwargs):\n",
        "    \"\"\"\n",
        "    Create a heatmap from a numpy array and two lists of labels.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data\n",
        "        A 2D numpy array of shape (M, N).\n",
        "    row_labels\n",
        "        A list or array of length M with the labels for the rows.\n",
        "    col_labels\n",
        "        A list or array of length N with the labels for the columns.\n",
        "    ax\n",
        "        A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If\n",
        "        not provided, use current Axes or create a new one.  Optional.\n",
        "    cbar_kw\n",
        "        A dictionary with arguments to `matplotlib.Figure.colorbar`.  Optional.\n",
        "    cbarlabel\n",
        "        The label for the colorbar.  Optional.\n",
        "    **kwargs\n",
        "        All other arguments are forwarded to `imshow`.\n",
        "    \"\"\"\n",
        "\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "\n",
        "    if cbar_kw is None:\n",
        "        cbar_kw = {}\n",
        "\n",
        "    if mask==True:\n",
        "        # Lower triangle mask\n",
        "        lower_mask = np.tri(data.shape[0], data.shape[1], k=-1)\n",
        "\n",
        "        # Upper triangle mask\n",
        "        upper_mask = lower_mask.T\n",
        "\n",
        "        # Mask the upper triangle\n",
        "        masked_data_upper = np.ma.array(data, mask=upper_mask)\n",
        "        # Plot the heatmap\n",
        "        im = ax.imshow(masked_data_upper, **kwargs)\n",
        "    else:\n",
        "        im = ax.imshow(data, **kwargs)\n",
        "\n",
        "    # Create colorbar\n",
        "    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n",
        "    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n",
        "\n",
        "    ax.set_xticks(np.arange(data.shape[1]), minor=False)\n",
        "    ax.set_yticks(np.arange(data.shape[0]), minor=False)\n",
        "\n",
        "    # Show all ticks and label them with the respective list entries.\n",
        "    ax.set_xticklabels(labels=col_labels,\n",
        "                  rotation=0, ha=\"center\", rotation_mode=\"anchor\", fontsize=9)\n",
        "    ax.set_yticklabels(labels=row_labels, fontsize=9)\n",
        "\n",
        "    # Let the horizontal axes labeling appear on top.\n",
        "    ax.tick_params(top=False, bottom=False,\n",
        "                   labeltop=False, labelbottom=True)\n",
        "\n",
        "    # Turn spines off and create white grid.\n",
        "    ax.spines[\"top\"].set_visible(False)\n",
        "    ax.spines[\"right\"].set_visible(False)\n",
        "    ax.spines[\"left\"].set_visible(False)\n",
        "    ax.spines[\"bottom\"].set_visible(False)\n",
        "\n",
        "    \n",
        "    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n",
        "    ax.minorticks_on()\n",
        "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
        "    # Only show minor gridlines once in between major gridlines.\n",
        "    ax.xaxis.set_minor_locator(AutoMinorLocator(2))\n",
        "    ax.yaxis.set_minor_locator(AutoMinorLocator(2))\n",
        "\n",
        "    return im, cbar\n",
        "\n",
        "\n",
        "def annotate_heatmap(im, data=None, valfmt=\"{x:.2f}\",\n",
        "                     textcolors=(\"black\", \"white\"),\n",
        "                     threshold=None, **textkw):\n",
        "    \"\"\"\n",
        "    A function to annotate a heatmap.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    im\n",
        "        The AxesImage to be labeled.\n",
        "    data\n",
        "        Data used to annotate.  If None, the image's data is used.  Optional.\n",
        "    valfmt\n",
        "        The format of the annotations inside the heatmap.  This should either\n",
        "        use the string format method, e.g. \"$ {x:.2f}\", or be a\n",
        "        `matplotlib.ticker.Formatter`.  Optional.\n",
        "    textcolors\n",
        "        A pair of colors.  The first is used for values below a threshold,\n",
        "        the second for those above.  Optional.\n",
        "    threshold\n",
        "        Value in data units according to which the colors from textcolors are\n",
        "        applied.  If None (the default) uses the middle of the colormap as\n",
        "        separation.  Optional.\n",
        "    **kwargs\n",
        "        All other arguments are forwarded to each call to `text` used to create\n",
        "        the text labels.\n",
        "    \"\"\"\n",
        "\n",
        "    if not isinstance(data, (list, np.ndarray)):\n",
        "        data = im.get_array()\n",
        "\n",
        "    # Normalize the threshold to the images color range.\n",
        "    if threshold is not None:\n",
        "        threshold = im.norm(threshold)\n",
        "    else:\n",
        "        threshold = im.norm(data.max())/2.\n",
        "\n",
        "    # Set default alignment to center, but allow it to be\n",
        "    # overwritten by textkw.\n",
        "    kw = dict(horizontalalignment=\"center\",\n",
        "              verticalalignment=\"center\")\n",
        "    kw.update(textkw)\n",
        "\n",
        "    # Get the formatter in case a string is supplied\n",
        "    if isinstance(valfmt, str):\n",
        "        valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)\n",
        "\n",
        "    # Loop over the data and create a `Text` for each \"pixel\".\n",
        "    # Change the text's color depending on the data.\n",
        "    texts = []\n",
        "    for i in range(data.shape[0]):\n",
        "        for j in range(data.shape[1]):\n",
        "            kw.update(color=textcolors[int(im.norm(data[i, j]) < threshold)])\n",
        "            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)\n",
        "            texts.append(text)\n",
        "\n",
        "    return texts"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "#sns.set_palette('mako')\n",
        "#sns.set_style(\"white\", {\"axes.edgecolor\": \".8\"})\n",
        "\n",
        "def pval_heatmap(arr, vec, metric, dataset):\n",
        "    \"\"\"\n",
        "    arr: array of p-values for the difference in model performance\n",
        "    vec: the models' classification scores\n",
        "    metric: \"mcc\" or \"auc\"\n",
        "    dataset: \"discharge\" or \"allnotes\"\n",
        "    \"\"\"\n",
        "\n",
        "    cmap = ListedColormap(sns.color_palette(\"mako\", n_colors=10))\n",
        "\n",
        "    fig, ax = plt.subplots(2,1, figsize=(6,6), gridspec_kw={'height_ratios': [4, 0.8]})\n",
        "\n",
        "    def make_stars(pval):\n",
        "        if pval < 0.0001:\n",
        "            return \"****\"\n",
        "        elif pval < 0.001:\n",
        "            return \"***\"\n",
        "        elif pval < 0.01:\n",
        "            return \"**\"\n",
        "        elif pval < 0.05:\n",
        "            return \"*\"\n",
        "        else:\n",
        "            return \"\"\n",
        "\n",
        "    fmt = matplotlib.ticker.FuncFormatter(lambda x, pos: make_stars(x))\n",
        "\n",
        "    x = [\"PsyRoBERTa\", \"RøBÆRTa\", \"MeDa-BERT\", \"BERT\", \"LR\"]\n",
        "    y = x\n",
        "\n",
        "    im, cbar = heatmap(arr, x, y, ax=ax[0],\n",
        "                    cmap=cmap, \n",
        "                    cbarlabel=\"p-value\"\n",
        "                    )\n",
        "    texts = annotate_heatmap(im, valfmt=fmt)\n",
        "\n",
        "    im2, cbar2 = heatmap(vec, [\"\"], y, ax=ax[1],\n",
        "                    cmap=cmap, \n",
        "                    #norm=,\n",
        "                    #cbar_kw=dict(ticks=np.arange(0, 1)), \n",
        "                    mask=False,\n",
        "                    cbarlabel=metric\n",
        "                    )\n",
        "    texts2 = annotate_heatmap(im2)\n",
        "\n",
        "    plt.savefig(f\"../../output/pval_{metric}_{dataset}_results.png\", bbox_inches=\"tight\")\n",
        "    plt.savefig(f\"../../output/pval_{metric}_{dataset}_results.pdf\", bbox_inches=\"tight\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pval_heatmap(arr1, np.array([[0.2849685192985258, \n",
        "                    0.22584728413447375,\n",
        "                    0.26413445007234954,\n",
        "                    0.21451372766702514,\n",
        "                    0.22276459614793295]]),\n",
        "                    \"MCC\",\n",
        "                    \"discharge\")\n",
        "\n",
        "#pval_heatmap(arr1, np.array([[0.7113156785886554,\n",
        "#                            0.6887363504523216,\n",
        "#                            0.7002935070331648,\n",
        "#                            0.6787924770709964,\n",
        "#                            0.6798484690301403]]),\n",
        "#                            \"AUC\",\n",
        "#                            \"discharge\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pval_heatmap(arr2, np.array([[0.3033716717576357,\n",
        "                                0.26337014027043776,\n",
        "                                0.29526487056699585,\n",
        "                                0.1927258837645364,\n",
        "                                0.25815224591611685]]),\n",
        "                                \"MCC\",\n",
        "                                \"allnotes\")\n",
        "\n",
        "#pval_heatmap(arr2, np.array([[0.7362246997573167,\n",
        "#                                0.7280923025865467,\n",
        "#                                0.734360125282612,\n",
        "#                                0.7178771027358901,\n",
        "#                                0.7182119848167431]]),\n",
        "#                                \"AUC\",\n",
        "#                                \"allnotes\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bootstrap_metric_distributions(metric, model_1_predictions, model_2_predictions, test_set_labels, n_resamples=1000):\n",
        "   \n",
        "    model_1_metric = calculate_metric(metric, model_1_predictions, test_set_labels)\n",
        "    model_2_metric = calculate_metric(metric, model_2_predictions, test_set_labels)\n",
        "    observed_difference = model_1_metric - model_2_metric\n",
        "\n",
        "    differences = np.empty(n_resamples)\n",
        "    n_samples = len(test_set_labels)\n",
        "\n",
        "    bootstrap_estimates_A = []\n",
        "    bootstrap_estimates_B = []\n",
        "    for i in range(n_resamples):\n",
        "        bootstrap_indices = np.random.choice(range(n_samples), size=n_samples, replace=True)\n",
        "        new_test_set_labels = test_set_labels[bootstrap_indices]\n",
        "        new_model_1_predictions = model_1_predictions[bootstrap_indices]\n",
        "        new_model_2_predictions = model_2_predictions[bootstrap_indices]\n",
        "\n",
        "        model_1_metric = calculate_metric(metric, new_model_1_predictions, new_test_set_labels)\n",
        "        model_2_metric = calculate_metric(metric, new_model_2_predictions, new_test_set_labels)\n",
        "        bootstrap_estimates_A.append(model_1_metric)\n",
        "        bootstrap_estimates_B.append(model_2_metric)\n",
        "        #differences[i] = model_1_metric - model_2_metric\n",
        "\n",
        "    #differences = differences - observed_difference\n",
        "    # Perform a two-sample test to test whether the mean metric of model A is equal to the mean metric of model B or not\n",
        "    # Use the scipy.stats.ttest_ind function\n",
        "    t_stat, p_value = scipy.stats.ttest_ind(bootstrap_estimates_A, bootstrap_estimates_B)\n",
        "    print(f\"The t-statistic is {t_stat:.3f} and the p-value is {p_value:.3f}\")\n",
        "\n",
        "    return bootstrap_estimates_A, bootstrap_estimates_B\n",
        "\n",
        "model_name1, m1, model_name2, m2 = model_names_discharge[0], models_discharge[0], model_names_discharge[2], models_discharge[2]\n",
        "EPOCH = 11\n",
        "\n",
        "res1 = m1[m1.epoch==EPOCH]\n",
        "temp_test1 = get_temp_df(res1, split=\"test\")\n",
        "\n",
        "res2 = m2[m2.epoch==EPOCH]\n",
        "temp_test2 = get_temp_df(res2, split=\"test\")\n",
        "\n",
        "print(model_name1, \"VS\", model_name2)\n",
        "print()\n",
        "\n",
        "print_model_results(model_name1, \"mcc\", temp_test1.p_mean, temp_test1.target)\n",
        "print_model_results(model_name2, \"mcc\", temp_test2.p_mean, temp_test2.target)\n",
        "\n",
        "distA, distB = calculate_bootstrap_metric_distributions(\"mcc\", temp_test1.p_mean, temp_test2.p_mean, temp_test1.target, n_resamples=1000)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.hist(distA, bins=100);\n",
        "ax.hist(distB, bins=100);"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "t_stat, p_value = scipy.stats.ttest_ind(distA, distB, equal_var='False')\n",
        "print(f\"The t-statistic is {t_stat:.3f} and the p-value is {p_value:.100f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}