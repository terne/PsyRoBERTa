{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm \n",
        "from transformers import AutoTokenizer\n",
        "import string\n",
        "sns.set_theme(style=\"whitegrid\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "directory = \"dischargesum\"#\"allnotes\"\n",
        "model_name = \"psyroberta_p4_epoch12\"#\"psyroberta_p4_dedupcont_epoch12\"\n",
        "model_path = \"../../finetuning/acutereadm_finetuned_models/\"\n",
        "\n",
        "if directory==\"dischargesum\":\n",
        "    text_column_name = \"text_names_removed_step2\"\n",
        "else:\n",
        "    text_column_name = \"DedupCont\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import MLClient#, Input, command\n",
        "from azure.identity import DefaultAzureCredential\n",
        "import sys\n",
        "sys.path.append(\"../..\")\n",
        "from utils import azure_ml_configs\n",
        "\n",
        "workspace_id = azure_ml_configs.workspace_id \n",
        "subscription_id = azure_ml_configs.subscription_id \n",
        "resource_group = azure_ml_configs.resource_group\n",
        "workspace_name = azure_ml_configs.workspace_name\n",
        "\n",
        "# Get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    credential=DefaultAzureCredential(),\n",
        "    subscription_id=subscription_id,\n",
        "    resource_group_name=resource_group,\n",
        "    workspace_name=workspace_name,\n",
        ")\n",
        "\n",
        "if directory==\"dischargesum\":\n",
        "    data_asset = ml_client.data.get(name=\"clinicalNote_AcuteReadmission\", version=1)\n",
        "else:\n",
        "    data_asset = ml_client.data.get(name=\"clinicalNote_AcuteReadmission_DedupCont\", version=1)\n",
        "\n",
        "print(f\"Data asset URI: {data_asset.path}\")\n",
        "data_path = data_asset.path\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path+directory+\"/\"+model_name, local_files_only=True)\n",
        "\n",
        "# loading and prepraring data\n",
        "cols = [text_column_name, \"Acute\", \"set\", \"Type\", \"PatientDurableKey\", \"EncounterKey\", \"CreationInstant\"]\n",
        "df = pd.read_csv(data_path, usecols=cols)\n",
        "# make sure the data is sorted by patient id, encounter and date\n",
        "df.sort_values(by=[\"PatientDurableKey\", \"EncounterKey\", \"CreationInstant\"],inplace=True)\n",
        "#rename main columns of interest\n",
        "df.rename(columns={text_column_name: \"text\", \"Acute\": \"label\"}, inplace=True)\n",
        "\n",
        "if directory==\"dischargesum\":\n",
        "    df = df[df[\"Type\"].str.contains(\"Udskrivningsresume|Udskrivningsresumé\")==True].copy()\n",
        "\n",
        "# concatenating texts on patient and encounter id\n",
        "df = df.groupby([\"PatientDurableKey\", \"EncounterKey\", \"label\", \"set\"]).text.apply(f'{tokenizer.sep_token}'.join).reset_index()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# load AR result\n",
        "result = pd.read_csv(f'../../result_files/{directory}_{model_name}_AR_train_results.csv')\n",
        "\n",
        "result['tokens_2'] = result.tokens.apply(lambda x: literal_eval(str(x)))\n",
        "result['attn_rollout_2'] = result.attn_rollout.apply(lambda x: literal_eval(str(x)))\n",
        "\n",
        "y_true = [df[df.EncounterKey==eid].label.item() for eid in result.eid.values]\n",
        "result[\"y_true\"] = y_true"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_token(token):\n",
        "    map_chars = {\"Â\": \"Ġ\",\n",
        "                 \"ł\": \"\",\n",
        "                 \"Ã¦\": \"æ\",\n",
        "                 \"Ã¸\":\"ø\",\n",
        "                 \"ãĺ\":\"ø\",\n",
        "                 \"Ã¥\":\"å\",\n",
        "                 \"âĢĿ\":\"\",\n",
        "                 \"Ã©\":\"é\",\n",
        "                 \"ãī\":\"é\"\n",
        "                }\n",
        "    \n",
        "    for k in map_chars.keys():\n",
        "        token = token.replace(k,map_chars[k])\n",
        "    \n",
        "    return token\n",
        "\n",
        "def only_punct_next(s):\n",
        "    return True if len([i for i in s if i in string.punctuation]) == len(s) else False\n",
        "    \n",
        "def merge_roberta_tokens(tokens, importance, method=\"sum\"):\n",
        "    # function adapted from https://github.com/beinborn/relative_importance/blob/main/extract_model_importance/tokenization_util.py\n",
        "    # and https://github.com/lautel/fair-rationales/blob/504aabb8f726df4372de45922facf949dd4fb044/src/extract_model_importance/tokenization_utils.py \n",
        "\n",
        "    # we don't care about pad tokens\n",
        "    pad_index = tokens.index(\"<pad>\") if \"<pad>\" in tokens else None\n",
        "    tokens = tokens[:pad_index]\n",
        "    importance = importance[:pad_index]\n",
        "    \n",
        "    adjusted_tokens = []\n",
        "    adjusted_importance = []\n",
        "    i = 1\n",
        "    # We ignore the last token \n",
        "    while i < len(tokens) - 1:\n",
        "        combined_token = clean_token(tokens[i])\n",
        "        \n",
        "        combined_heat = importance[i]\n",
        "        combined_heat_max = [importance[i]]\n",
        "        \n",
        "        # keep track how number of tokens combined\n",
        "        combine_count=1\n",
        "        \n",
        "        # check if token is only punctuation\n",
        "        only_punct = only_punct_next(combined_token.replace(\"Ġ\", \"\"))\n",
        "        \n",
        "        # Nothing to be done for the first and last token\n",
        "        if 0< i < (len(tokens) - 2):\n",
        "            #next_token = clean_token(tokens[i + 1])\n",
        "    \n",
        "            #while not clean_token(tokens[i + 1]).startswith(\"Ġ\") and tokens[i + 1] not in string.punctuation and not only_punct:\n",
        "            while not clean_token(tokens[i + 1]).startswith(\"Ġ\") and not only_punct_next(tokens[i + 1]) and not only_punct:\n",
        "                combined_token = combined_token + clean_token(tokens[i + 1])\n",
        "                combined_heat = combined_heat + importance[i + 1]\n",
        "                combined_heat_max.append(importance[i + 1])\n",
        "                combine_count += 1\n",
        "                i += 1\n",
        "                if i == len(tokens) - 2:\n",
        "                    break\n",
        "        combined_token = combined_token.replace(\"Ġ\", \"\")\n",
        "        # as there are some errors in the placement of the Name token, we disregard these tokens\n",
        "        if len(combined_token)>0 and \"[Name]\" not in combined_token and \"</s>\" not in combined_token:\n",
        "            # we lowercase the combined token before appending\n",
        "            adjusted_tokens.append(combined_token.lower())\n",
        "            # optional: take mean of attention over combined tokens:\n",
        "            if method==\"mean\":\n",
        "                adjusted_importance.append(combined_heat/combine_count)\n",
        "            # otherwise take sum of attentions:\n",
        "            elif method==\"sum\":\n",
        "                adjusted_importance.append(combined_heat)\n",
        "            # or take the max:\n",
        "            elif method==\"max\":\n",
        "                adjusted_importance.append(max(combined_heat_max))\n",
        "            \n",
        "        i += 1\n",
        "        \n",
        "    assert len(adjusted_tokens)==len(adjusted_importance)\n",
        "    return adjusted_tokens, adjusted_importance"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = []\n",
        "all_importances = []\n",
        "for i in tqdm(range(len(result))):\n",
        "    t,v = merge_roberta_tokens(result.tokens_2.values[i], result.attn_rollout_2.values[i], method=\"max\")\n",
        "    all_tokens.append(t)\n",
        "    all_importances.append(v)\n",
        "\n",
        "result[\"merged_tokens\"] = all_tokens\n",
        "result[\"merged_attentions\"] = all_importances\n",
        "\n",
        "result['merged_tokens'] = result.merged_tokens.apply(lambda x: literal_eval(str(x)))\n",
        "result['merged_attentions'] = result.merged_attentions.apply(lambda x: literal_eval(str(x)))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def count_token_patient_freq(result_df):\n",
        "    # group token lists on patient id\n",
        "    result_gb = result_df.groupby([\"pid\"]).agg({\"merged_tokens\":\"sum\"})\n",
        "    \n",
        "    toks = []\n",
        "    for i in range(len(result_gb)):\n",
        "        toks+=result_gb.merged_tokens.values[i]\n",
        "    \n",
        "    tok_set = set(toks)\n",
        "    \n",
        "    token_pt_freq = defaultdict(int)\n",
        "    \n",
        "    for i in tqdm(range(len(result_gb))):\n",
        "        to_count = tok_set.intersection(set(result_gb.merged_tokens.values[i]))\n",
        "        for t in to_count:\n",
        "            token_pt_freq[t] += 1\n",
        "\n",
        "    return token_pt_freq\n",
        "\n",
        "def sort_attentions(result_df):\n",
        "    # one list of all tokens\n",
        "    toks = []\n",
        "    for i in range(len(result_df)):\n",
        "        toks+=result_df.merged_tokens.values[i]\n",
        "    \n",
        "    # one list of all attentions\n",
        "    attentions = []\n",
        "    for i in range(len(result_df)):\n",
        "        attentions+=result_df.merged_attentions.values[i]\n",
        "    \n",
        "    frequency_dict = defaultdict(int)\n",
        "    for t in toks:\n",
        "        frequency_dict[t]+=1\n",
        "        \n",
        "    mean_attention_dict = defaultdict(float)\n",
        "\n",
        "    for t,a in list(zip(toks,attentions)):\n",
        "        mean_attention_dict[t] += a\n",
        "\n",
        "    for k in mean_attention_dict.keys():\n",
        "        mean_attention_dict[k] /= frequency_dict[k]\n",
        "    \n",
        "    freq_p_dict = count_token_patient_freq(result_df)\n",
        "        \n",
        "    d = {\"words\": mean_attention_dict.keys(), \n",
        "         \"attention\": mean_attention_dict.values(), \n",
        "         \"freq\": [frequency_dict[k] for k in mean_attention_dict.keys()],\n",
        "         \"freq_p\": [freq_p_dict[k] for k in mean_attention_dict.keys()]}\n",
        "    \n",
        "    att_df = pd.DataFrame.from_dict(d)\n",
        "    return att_df\n",
        "\n",
        "def find_ngrams(input_list, n):\n",
        "    return list(zip(*[input_list[i:] for i in range(n)]))\n",
        "\n",
        "def ngram_attentions(result_df, n):\n",
        "    \n",
        "    result_df = result_df.groupby([\"pid\"]).agg({\"merged_tokens\":\"sum\", \n",
        "                                                \"merged_attentions\":\"sum\"})\n",
        "    \n",
        "    frequency_dict = defaultdict(int)\n",
        "    mean_attention_dict = defaultdict(float)\n",
        "    \n",
        "    ngrams = []\n",
        "    ngrams_att = []\n",
        "    for i in range(len(result_df)):\n",
        "        toks = result_df.merged_tokens.values[i]\n",
        "        attentions = result_df.merged_attentions.values[i]\n",
        "        curr_ngrams = [\" \".join(x) for x in find_ngrams(toks, n)]\n",
        "        curr_ngrams_att = list(map(sum,find_ngrams(attentions,n)))\n",
        "        ngrams += curr_ngrams\n",
        "        ngrams_att += curr_ngrams_att\n",
        "    \n",
        "    assert len(ngrams)==len(ngrams_att)\n",
        "    \n",
        "    for t in ngrams:\n",
        "        frequency_dict[t] +=1\n",
        "        \n",
        "    for t,a in list(zip(ngrams,ngrams_att)):\n",
        "        mean_attention_dict[t] += a\n",
        "\n",
        "    for k in mean_attention_dict.keys():\n",
        "        mean_attention_dict[k] /= frequency_dict[k]\n",
        "    \n",
        "    n_grams_set = set(ngrams)\n",
        "    \n",
        "    ngram_pt_freq = defaultdict(int)\n",
        "    \n",
        "    for i in tqdm(range(len(result_df))):\n",
        "        curr_n_grams = [\" \".join(x) for x in find_ngrams(result_df.merged_tokens.values[i], n)] \n",
        "\n",
        "        to_count = n_grams_set.intersection(set(curr_n_grams))\n",
        "        \n",
        "        for t in to_count:\n",
        "            ngram_pt_freq[t] += 1\n",
        "            \n",
        "    d = {\"words\": mean_attention_dict.keys(), \n",
        "         \"attention\": mean_attention_dict.values(), \n",
        "         \"freq\": [frequency_dict[k] for k in mean_attention_dict.keys()],\n",
        "         \"freq_p\": [ngram_pt_freq[k] for k in mean_attention_dict.keys()]}\n",
        "    \n",
        "    att_df = pd.DataFrame.from_dict(d)\n",
        "    return att_df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def plotting_simple(highdf, \n",
        "             n, \n",
        "             method, \n",
        "             freq_rule=\"top 10% most frequent\"):\n",
        "    \n",
        "    print(freq_rule, n)\n",
        "    \n",
        "    palette_name1 = 'mako'#\"Reds_r\"\n",
        "    #palette_name2 = \"Blues\"\n",
        "    sns.set(font_scale=0.8,style=\"whitegrid\")\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(3,5))\n",
        "\n",
        "    sns.barplot(x=\"attention\", y=\"words\", data=highdf,\n",
        "                label=\"Largest attention all tokens\" , ax=ax,\n",
        "               palette=palette_name1)\n",
        "    ax.xaxis.label.set_visible(False)\n",
        "    ax.yaxis.label.set_visible(False)\n",
        "    ax.set_title(\"Attention\")\n",
        "\n",
        "def plotting(highdf, \n",
        "             n, \n",
        "             method, \n",
        "             freq_rule=\"top 10% most frequent\", \n",
        "             xlim_lst = None,\n",
        "             annot=\"a\"):\n",
        "    \n",
        "    print(freq_rule)\n",
        "    \n",
        "    palette_name1 = 'mako'\n",
        "    \n",
        "    sns.set(font_scale=0.8,style=\"whitegrid\")\n",
        "\n",
        "    fig = plt.figure(figsize=(8,5))\n",
        "    \n",
        "    \n",
        "    if xlim_lst==None:\n",
        "        gs = GridSpec(1, 4, figure=fig)\n",
        "        \n",
        "        ax1 = fig.add_subplot(gs[0,0 :2])\n",
        "        ax1.xaxis.label.set_visible(False)\n",
        "        ax1.yaxis.label.set_visible(False)\n",
        "        \n",
        "        \n",
        "        # freq\n",
        "        ax2 = fig.add_subplot(gs[0,2 :3], sharey=ax1)\n",
        "        plt.setp(ax2.get_yticklabels(), visible=False)\n",
        "        ax2.yaxis.label.set_visible(False)\n",
        "        ax2.xaxis.label.set_visible(False)\n",
        "        \n",
        "        # freq_p\n",
        "        ax3 = fig.add_subplot(gs[0,3 :4], sharey=ax1)\n",
        "        plt.setp(ax3.get_yticklabels(), visible=False)\n",
        "        ax3.yaxis.label.set_visible(False)\n",
        "        ax3.xaxis.label.set_visible(False)\n",
        "        \n",
        "        ax1.set_title(\"Attention\")\n",
        "        ax2.set_title(\"Word frequency\")\n",
        "        ax3.set_title(\"Patient frequency\\nof word occurrence\")\n",
        "        \n",
        "    else:\n",
        "        gs = GridSpec(1, 12, figure=fig)\n",
        "        \n",
        "        ax1 = fig.add_subplot(gs[0,0 :6])\n",
        "        ax1.xaxis.label.set_visible(False)\n",
        "        ax1.yaxis.label.set_visible(False)\n",
        "        \n",
        "        # freq\n",
        "        ax2 = fig.add_subplot(gs[0,6 :8], sharey=ax1)\n",
        "        plt.setp(ax2.get_yticklabels(), visible=False)\n",
        "        ax2.yaxis.label.set_visible(False)\n",
        "        ax2.xaxis.label.set_visible(False)\n",
        "        \n",
        "        ax2b = fig.add_subplot(gs[0,8 :9], sharey=ax1)\n",
        "        plt.setp(ax2b.get_yticklabels(), visible=False)\n",
        "        ax2b.yaxis.label.set_visible(False)\n",
        "        ax2b.xaxis.label.set_visible(False)\n",
        "        \n",
        "        ax2.set_xlim(xlim_lst[0],xlim_lst[1])\n",
        "        ax2b.set_xlim(xlim_lst[2],xlim_lst[3])\n",
        "\n",
        "        \n",
        "        # freq_p\n",
        "        ax3 = fig.add_subplot(gs[0,9 :11], sharey=ax1)\n",
        "        plt.setp(ax3.get_yticklabels(), visible=False)\n",
        "        ax3.yaxis.label.set_visible(False)\n",
        "        ax3.xaxis.label.set_visible(False)\n",
        "        ax3b = fig.add_subplot(gs[0,11 :12], sharey=ax1)\n",
        "        plt.setp(ax3b.get_yticklabels(), visible=False)\n",
        "        ax3b.yaxis.label.set_visible(False)\n",
        "        ax3b.xaxis.label.set_visible(False)\n",
        "        \n",
        "        ax3.set_xlim(xlim_lst[4],xlim_lst[5])\n",
        "        ax3b.set_xlim(xlim_lst[6],xlim_lst[7])\n",
        "        \n",
        "\n",
        "    highdf[\"words\"] = highdf[\"words\"].apply(lambda x: x.replace(\"schaumburg\", \"[name]\"))\n",
        "\n",
        "    sns.barplot(x=\"attention\", y=\"words\", data=highdf,\n",
        "                label=\"Largest attention all tokens\" , ax=ax1,\n",
        "               palette=palette_name1)\n",
        "\n",
        "    sns.barplot(x=\"freq\", y=\"words\", data=highdf,\n",
        "                label=\"\" , ax=ax2, #\n",
        "               palette=palette_name1)\n",
        "    \n",
        "    sns.barplot(x=\"freq_p\", y=\"words\", data=highdf,\n",
        "                label=\"\" , ax=ax3, #\n",
        "               palette=palette_name1)\n",
        "    \n",
        "    if not xlim_lst==None:\n",
        "        sns.barplot(x=\"freq\", y=\"words\", data=highdf,\n",
        "                label=\"\" , ax=ax2b, #\n",
        "               palette=palette_name1)\n",
        "    \n",
        "        sns.barplot(x=\"freq_p\", y=\"words\", data=highdf,\n",
        "                label=\"\" , ax=ax3b, #\n",
        "               palette=palette_name1)\n",
        "    \n",
        "        d = .015  # how big to make the diagonal lines in axes coordinates\n",
        "        # arguments to pass plot, just so we don't keep repeating them\n",
        "        kwargs = dict(transform=ax2.transAxes, color='grey', clip_on=False)\n",
        "        ax2.plot((1-d, 1+d), (-d, +d), **kwargs, zorder=10)\n",
        "        ax2.plot((1-d, 1+d), (1-d, 1+d), **kwargs, zorder=10)\n",
        "        \n",
        "        kwargs.update(transform=ax2b.transAxes)  # switch to the bottom axes\n",
        "        ax2b.plot((-d, +d), (1-d, 1+d), **kwargs, zorder=10)\n",
        "        ax2b.plot((-d, +d), (-d, +d), **kwargs, zorder=10)\n",
        "        \n",
        "        # hide the spines between ax and ax2\n",
        "        ax2.spines['right'].set_visible(False)\n",
        "        ax2b.spines['left'].set_visible(False)\n",
        "        \n",
        "        kwargs = dict(transform=ax3.transAxes, color='grey', clip_on=False)\n",
        "        ax3.plot((1-d, 1+d), (-d, +d), **kwargs, zorder=10)\n",
        "        ax3.plot((1-d, 1+d), (1-d, 1+d), **kwargs, zorder=10)\n",
        "        kwargs.update(transform=ax3b.transAxes)  # switch to the bottom axes\n",
        "        ax3b.plot((-d, +d), (1-d, 1+d), **kwargs, zorder=10)\n",
        "        ax3b.plot((-d, +d), (-d, +d), **kwargs, zorder=10)\n",
        "        \n",
        "        # hide the spines between ax and ax2\n",
        "        ax3.spines['right'].set_visible(False)\n",
        "        ax3b.spines['left'].set_visible(False)\n",
        "        \n",
        "        plt.setp(ax1.get_xticklabels(), rotation=45, rotation_mode=\"anchor\")\n",
        "        plt.setp(ax2.get_xticklabels(), rotation=45, rotation_mode=\"anchor\")\n",
        "        plt.setp(ax2b.get_xticklabels(), rotation=45, rotation_mode=\"anchor\")\n",
        "        plt.setp(ax3.get_xticklabels(), rotation=45, rotation_mode=\"anchor\")\n",
        "        plt.setp(ax3b.get_xticklabels(), rotation=45, rotation_mode=\"anchor\")\n",
        "        \n",
        "        \n",
        "        ghost1 = fig.add_subplot(gs[:6],label=\"attention title\")\n",
        "        ghost1.axis('off')\n",
        "        ghost1.set_title(\"Attention\")\n",
        "\n",
        "        ghost2 = fig.add_subplot(gs[0,6:9], label=\"freq title\")\n",
        "        ghost2.axis('off')\n",
        "        ghost2.set_title(\"Word frequency\")\n",
        "        \n",
        "        ghost3 = fig.add_subplot(gs[0,9:12], label=\"freq_p title\")\n",
        "        ghost3.axis('off')\n",
        "        ghost3.set_title(\"Patient frequency\\nof word occurrence\")\n",
        "        \n",
        "    \n",
        "    fig.tight_layout()\n",
        "    \n",
        "    if not xlim_lst==None:\n",
        "        fig.subplots_adjust(wspace=1, hspace=1)\n",
        "\n",
        "    \n",
        "    \n",
        "def plot_attentions(result, method=\"max\", n=1):\n",
        "    \n",
        "    all_tokens = []\n",
        "    all_importances = []\n",
        "    for i in tqdm(range(len(result))):\n",
        "        t,v = merge_roberta_tokens(result.tokens_2.values[i], result.attn_rollout_2.values[i], method=method)\n",
        "        all_tokens.append(t)\n",
        "        all_importances.append(v)\n",
        "\n",
        "    result[\"merged_tokens\"] = all_tokens\n",
        "    result[\"merged_attentions\"] = all_importances\n",
        "\n",
        "    result['merged_tokens'] = result.merged_tokens.apply(lambda x: literal_eval(str(x)))\n",
        "    result['merged_attentions'] = result.merged_attentions.apply(lambda x: literal_eval(str(x)))\n",
        "    \n",
        "    result_highrisk = result[result.pos_prob>=0.7].copy()\n",
        "    \n",
        "    att_df = ngram_attentions(result, n)\n",
        "    att_df_highrisk = ngram_attentions(result_highrisk, n)    \n",
        "    \n",
        "    all_freq = att_df[att_df.freq_p>=3].sort_values(by=[\"attention\"],ascending=False)[:25]\n",
        "\n",
        "    plotting(all_freq,n=n,method=method,freq_rule=\"all\")\n",
        "    \n",
        "    k=10\n",
        "    top_k = int(len(att_df)*(k/100))\n",
        "    print(top_k)\n",
        "    \n",
        "    top_k_freq = att_df[att_df.freq_p>=3].sort_values(by=[\"freq\"], ascending=False)[:top_k]\n",
        "    top_k_freq_high = top_k_freq.sort_values(by=[\"attention\"],ascending=False)[:25]\n",
        "    plotting(top_k_freq_high,n=n, method=method)\n",
        "    \n",
        "    top_k_freq_highrisk = att_df_highrisk[(att_df_highrisk.words.isin(top_k_freq.words))&(att_df_highrisk.freq_p>=3)].sort_values(by=[\"attention\"],ascending=False)[:25]\n",
        "    \n",
        "    plotting(top_k_freq_highrisk,n=n, method=method)\n",
        "    \n",
        "    print(n, method)\n",
        "    print(\"With 10% most freqent:\")\n",
        "    for w in top_k_freq_high.words.values:\n",
        "        print(w)\n",
        "    for a in top_k_freq_high.attention.values:\n",
        "        print(np.round(a,3))\n",
        "    \n",
        "    \n",
        "    print(\"With all words:\")\n",
        "    for w in all_freq.words.values:\n",
        "        print(w)\n",
        "    for a in all_freq.attention.values:\n",
        "        print(np.round(a,3))\n",
        "    \n",
        "    \n",
        "    print(\"highrisk with 10% most freqeunt words (globally in set)\")\n",
        "    for w in top_k_freq_highrisk.words.values:\n",
        "        print(w)\n",
        "    for a in top_k_freq_highrisk.attention.values:\n",
        "        print(np.round(a,3))\n",
        "    \n",
        "    return all_freq, top_k_freq_high, top_k_freq_highrisk"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of extracting attentions on trigrams and using the max attention score (mean and sum are also possible)\n",
        "\n",
        "all_freq, top_k_freq_high, top_k_freq_highrisk = plot_attentions(result, method=\"max\", n=3)\n",
        "\n",
        "plotting_simple(top_k_freq_high, \n",
        "             3, \n",
        "             method=\"max\", \n",
        "             freq_rule=\"top 10% most frequent\",)\n",
        "#plt.savefig(\"train_top10_max_trigram.pdf\", bbox_inches=\"tight\")\n",
        "#plt.savefig(\"train_top10_max_trigram.png\", bbox_inches=\"tight\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}