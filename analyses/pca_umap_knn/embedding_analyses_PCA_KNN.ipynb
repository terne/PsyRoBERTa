{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import azure-core elements\n",
        "import azureml.core\n",
        "from azureml.core.workspace import Workspace\n",
        "from azureml.core import ScriptRunConfig, Environment, Experiment\n",
        "from azureml.core.environment import CondaDependencies\n",
        "from azureml.core import Workspace, Datastore, Dataset\n",
        "from azureml.data.dataset_factory import DataType\n",
        "\n",
        "# Initiate workspace\n",
        "workspace = Workspace.from_config()\n",
        "\n",
        "# Define datastore and load dataset\n",
        "datastore_name = 'sp_data'\n",
        "datastore = Datastore.get(workspace, datastore_name)\n",
        "\n",
        "datastore_paths = [(datastore, '/patients.parquet')] \n",
        "ds = Dataset.Tabular.from_parquet_files(path=datastore_paths)\n",
        "dfP = ds.to_pandas_dataframe()\n",
        "dfP.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "#import colorcet as cc\n",
        "from datasets import Dataset, DatasetDict\n",
        "sns.set(style=\"whitegrid\")\n",
        "sns.set_palette('mako_r')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Set path for the finetuned model\n",
        "model_path = \"../../finetuning/acutereadm_finetuned_models/dischargesum/\"\n",
        "model_name = \"psyroberta_p4_epoch12\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = AutoModel.from_pretrained(model_path+model_name, \n",
        "                                    local_files_only=True,\n",
        "                                    use_safetensors=True, \n",
        "                                    output_hidden_states=True,\n",
        "                                    output_attentions=True)#.cuda()\n",
        "model.to(device)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path+model_name, local_files_only=True)\n",
        "\n",
        "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "events = pd.read_parquet(\"../../data/acuteReadmission/events_acute_labels.parquet\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "# make new encounter column, as acute and non acute have different encounter columns in events dataframe\n",
        "events[\"encounter\"] = [int(i) if math.isnan(i)==False else int(j) for (i,j) in list(zip(events.EncounterKey_dis.values, events.EncounterKey.values))]\n",
        "\n",
        "event = events.merge(\n",
        "        dfP[[\"DurableKey\", \"BirthDate\"]],\n",
        "        left_on = 'PatientDurableKey',\n",
        "        right_on = 'DurableKey')\\\n",
        "        .drop(columns='DurableKey')\n",
        "event['Age'] = np.floor((pd.to_datetime(event.Date_dis) -pd.to_datetime(event.BirthDate)).dt.days / 365.25).astype(int)\n",
        "\n",
        "age_df = event[[\"encounter\", \"Age\"]].copy()\n",
        "event.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import MLClient#, Input, command\n",
        "from azure.identity import DefaultAzureCredential\n",
        "import sys\n",
        "sys.path.append(\"../..\")\n",
        "import azure_ml_configs\n",
        "\n",
        "workspace_id = azure_ml_configs.workspace_id \n",
        "subscription_id = azure_ml_configs.subscription_id \n",
        "resource_group = azure_ml_configs.resource_group\n",
        "workspace_name = azure_ml_configs.workspace_name\n",
        "\n",
        "# Get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    credential=DefaultAzureCredential(),\n",
        "    subscription_id=subscription_id,\n",
        "    resource_group_name=resource_group,\n",
        "    workspace_name=workspace_name,\n",
        ")\n",
        "\n",
        "discharge_notes_only = True\n",
        "text_column_name = \"text_names_removed_step2\"\n",
        "\n",
        "data_asset = ml_client.data.get(name=\"clinicalNote_AcuteReadmission\", version=1)\n",
        "\n",
        "print(f\"Data asset URI: {data_asset.path}\")\n",
        "\n",
        "data_path = data_asset.path"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# loading and prepraring data\n",
        "cols = [text_column_name, \"Acute\", \"set\", \"Type\", \"PatientDurableKey\", \"EncounterKey\", \"CreationInstant\"]\n",
        "df = pd.read_csv(data_path, usecols=cols)\n",
        "# make sure the data is sorted by patient id, encounter and date\n",
        "df.sort_values(by=[\"PatientDurableKey\", \"EncounterKey\", \"CreationInstant\"],inplace=True)\n",
        "#rename main columns of interest\n",
        "df.rename(columns={text_column_name: \"text\", \"Acute\": \"label\"}, inplace=True)\n",
        "\n",
        "if discharge_notes_only:\n",
        "    df = df[df[\"Type\"].str.contains(\"Udskrivningsresume|Udskrivningsresum√©\")==True].copy()\n",
        "    \n",
        "\n",
        "# concatenating texts on patient and encounter id\n",
        "df = df.groupby([\"PatientDurableKey\", \"EncounterKey\", \"label\", \"set\"]).text.apply(f'{tokenizer.sep_token}'.join).reset_index()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "data_dict = {\n",
        "    \"train\": Dataset.from_pandas(df[df.set==\"train\"]),\n",
        "    \"validation\": Dataset.from_pandas(df[df.set==\"val\"]),\n",
        "    \"test\": Dataset.from_pandas(df[df.set==\"test\"])\n",
        "    }\n",
        "\n",
        "\n",
        "raw_datasets = DatasetDict(data_dict)\n",
        "\n",
        "text_column_name = \"text\"\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    labs = []\n",
        "    patientids = []\n",
        "    encounterids = []\n",
        "    texts = []\n",
        "    for x,y, patient_id, encounter_id in list(zip(examples[\"text\"], examples[\"label\"], examples[\"PatientDurableKey\"], examples[\"EncounterKey\"])):\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            x,  # Sentence to encode\n",
        "            add_special_tokens=True,  # Add '[CLS]' and '[SEP]' or equivelant for roberta\n",
        "            max_length=512,  # Pad & truncate all sentences.\n",
        "            padding=\"max_length\", #(needing to specify truncation=True depends on version)\n",
        "            truncation=True,\n",
        "            return_overflowing_tokens=True, # return lists of tokens above 512 \n",
        "            return_offsets_mapping=True,\n",
        "            stride=32, # The stride used when the context is too large and is split across several features.\n",
        "            return_attention_mask=True,  # Construct attn. masks.\n",
        "            return_tensors='pt'  # Return pytorch tensors.\n",
        "        )\n",
        "        for inputs, attentions in list(zip(encoded_dict['input_ids'],encoded_dict['attention_mask']))[:None]:\n",
        "            #print(i.shape)\n",
        "            # Add the encoded sentence to the list.\n",
        "            input_ids.append(inputs)\n",
        "            texts.append(tokenizer.decode(inputs))\n",
        "            #And its attention mask (simply differentiates padding from non-padding).\n",
        "            attention_masks.append(attentions)\n",
        "            labs.append(y)\n",
        "            patientids.append(patient_id)\n",
        "            encounterids.append(encounter_id)\n",
        "    assert len(input_ids) == len(attention_masks) == len(labs) == len(patientids) == len(encounterids)\n",
        "    sample = {\"inputs\": input_ids,\n",
        "            \"attn_masks\": attention_masks,\n",
        "            \"labels\": labs,\n",
        "            \"patient_id\": patientids,\n",
        "            \"encounter_id\": encounterids,\n",
        "            \"text_split\":texts}\n",
        "    return sample\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "            tokenize_function,\n",
        "            batched=True,\n",
        "            num_proc=None,\n",
        "            remove_columns=raw_datasets['validation'].column_names,\n",
        "            #load_from_cache_file=not args.overwrite_cache,\n",
        "            desc=\"Running tokenizer on every text in dataset\",\n",
        "        )\n",
        "\n",
        "tokenized_datasets[\"train\"].set_format(type='pt', columns=['inputs', 'attn_masks', 'labels', 'patient_id', 'encounter_id'])\n",
        "tokenized_datasets[\"validation\"].set_format(type='pt', columns=['inputs', 'attn_masks', 'labels', 'patient_id', 'encounter_id'])\n",
        "#if args.on_test:\n",
        "tokenized_datasets[\"test\"].set_format(type='pt',columns=['inputs', 'attn_masks', 'labels', 'patient_id', 'encounter_id'])\n",
        "\n",
        "traindata = tokenized_datasets[\"train\"]\n",
        "valdata = tokenized_datasets[\"validation\"]\n",
        "#if args.on_test:\n",
        "testdata = tokenized_datasets[\"test\"]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def embedding_extraction(data):\n",
        "    # adapted from: \n",
        "    #https://medium.com/@minamehdinia213/fine-tuned-bert-embeddings-and-t-sne-visualization-bdfd09563744\n",
        "    \n",
        "    \n",
        "    # Store embeddings\n",
        "    cls_embeddings = []\n",
        "    mean_pooled_embeddings = []\n",
        "    preds = []\n",
        "    pos_probs = []\n",
        "\n",
        "    # Extract input data from the val dataset\n",
        "    input_ids = data[\"inputs\"]\n",
        "    attention_mask = data[\"attn_masks\"]\n",
        "    #labels = valdata[\"labels\"]\n",
        "    #eids = valdata[\"encounter_id\"]\n",
        "\n",
        "    # Convert input data to tensors and move them to the same device as the model\n",
        "    input_ids = torch.tensor(input_ids).to(device)\n",
        "    attention_mask = torch.tensor(attention_mask).to(device)\n",
        "    \n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_id, attention_mask in tqdm(zip(input_ids, attention_mask), total=len(input_ids)):\n",
        "            \n",
        "            # Forward pass, return hidden states\n",
        "            outputs = model(input_ids=input_id.unsqueeze(0).to(device), \n",
        "                        attention_mask=attention_mask.unsqueeze(0).to(device), \n",
        "                        output_hidden_states=True)\n",
        "            \n",
        "            probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1).detach().cpu()\n",
        "            prediction = np.argmax(probabilities, axis=1).flatten()\n",
        "            pos_prob = probabilities[:,1:].flatten()\n",
        "            preds.append(prediction)\n",
        "            pos_probs.append(pos_prob)\n",
        "            \n",
        "        \n",
        "            # Extract embeddings from the hidden states\n",
        "            hidden_states = outputs.hidden_states\n",
        "            last_hidden_state = hidden_states[-1]  # The last layer hidden state\n",
        "\n",
        "            # [CLS] token embeddings\n",
        "            cls_embedding = last_hidden_state[:, 0, :].detach().cpu().numpy()\n",
        "            cls_embeddings.append(cls_embedding.flatten())\n",
        "\n",
        "            # Mean pooling\n",
        "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
        "            sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
        "            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "            mean_pooled_embedding = (sum_embeddings / sum_mask).detach().cpu().numpy()\n",
        "            mean_pooled_embeddings.append(mean_pooled_embedding.flatten())\n",
        "\n",
        "    #convert to numpy array\n",
        "    cls_embeddings = np.array(cls_embeddings)\n",
        "    mean_pooled_embeddings = np.array(mean_pooled_embeddings)\n",
        "    return cls_embeddings, mean_pooled_embeddings, preds"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "test_cls_embeddings, test_mean_pooled_embeddings, test_predictions = embedding_extraction(testdata)\n",
        "np.save(\"test_cls_embeddings.npy\", test_cls_embeddings)\n",
        "np.save(\"test_mean_pooled_embeddings.npy\", test_mean_pooled_embeddings)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_cls_embeddings, train_mean_pooled_embeddings, val_predictions = embedding_extraction(traindata)\n",
        "np.save(\"train_cls_embeddings.npy\", train_cls_embeddings)\n",
        "np.save(\"train_mean_pooled_embeddings.npy\", train_mean_pooled_embeddings)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# possibly load embeddings here\n",
        "\n",
        "train_cls_embeddings = np.load(\"train_cls_embeddings.npy\")\n",
        "train_mean_pooled_embeddings = np.load(\"train_mean_pooled_embeddings.npy\")\n",
        "\n",
        "test_cls_embeddings = np.load(\"test_cls_embeddings.npy\")\n",
        "test_mean_pooled_embeddings = np.load(\"test_mean_pooled_embeddings.npy\")\n",
        "\n",
        "val_cls_embeddings = np.load(\"val_cls_embeddings.npy\")\n",
        "val_mean_pooled_embeddings = np.load(\"val_mean_pooled_embeddings.npy\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def agegroups(x):\n",
        "    if x==\"Unknown\":\n",
        "        return x\n",
        "    elif x < 18:\n",
        "        return \"Children\"\n",
        "    elif x >= 18 and x < 35:\n",
        "        return \"Young adults\"\n",
        "    elif x >= 35 and x < 55:\n",
        "        return \"Adults\"\n",
        "    else:\n",
        "        return \"Seniors\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "preds = pd.read_csv(\"../../result_files/dischargesum_psyroberta_p4_epoch12_AR_train_results.csv\")\n",
        "preds = preds[[\"pos_prob\", \"pid\", \"eid\"]].copy()\n",
        "preds = preds.groupby(by=[\"eid\",\"pid\"])[\"pos_prob\"].mean().reset_index()\n",
        "preds[\"pred\"] = preds.pos_prob.apply(lambda x: 0 if x<0.5 else 1)\n",
        "\n",
        "train_labels = traindata[\"labels\"]\n",
        "train_eids = traindata[\"encounter_id\"]\n",
        "train_pids = traindata[\"patient_id\"]\n",
        "train_encounter_embs_cls = np.array([np.array([train_cls_embeddings[i] for i in np.where(np.array(train_eids) == k)[0]]).mean(axis=0) for k in np.unique(train_eids)])\n",
        "#train_num_notes_in_encounter = [len([train_cls_embeddings[i] for i in np.where(np.array(train_eids) == k)[0]]) for k in np.unique(train_eids)]\n",
        "train_encounter_embs_mean_pooled = np.array([np.array([train_mean_pooled_embeddings[i] for i in np.where(np.array(train_eids) == k)[0]]).mean(axis=0) for k in np.unique(train_eids)])\n",
        "train_encounter_labs = np.array([np.array([train_labels[i] for i in np.where(np.array(train_eids) == k)[0]]).mean(axis=0) for k in np.unique(train_eids)])\n",
        "train_num_notes_in_encounter = np.array([len([train_labels[i] for i in np.where(np.array(train_eids) == k)[0]]) for k in np.unique(train_eids)])\n",
        "train_encounter_pid = np.array([[train_pids[i].item() for i in np.where(np.array(train_eids) == k)[0]][0] for k in np.unique(train_eids)])\n",
        "\n",
        "train_probs = np.array([preds[preds.eid==i].pos_prob.item() for i in np.unique(train_eids)])\n",
        "train_preds = np.array([preds[preds.eid==i].pred.item() for i in np.unique(train_eids)])\n",
        "#train_encounter_tokens =  []\n",
        "\n",
        "intersection_train = set(train_encounter_pid.tolist()).intersection(set(dfP.DurableKey.values.tolist()))\n",
        "train_sex = [dfP[dfP[\"DurableKey\"]==i].Sex.item() if i in intersection_train else \"Unknown\" for i in train_encounter_pid]\n",
        "train_eth = [dfP[dfP[\"DurableKey\"]==i].Ethnicity.item() if i in intersection_train else \"Unknown\" for i in train_encounter_pid]\n",
        "train_age = [age_df[age_df.encounter==i].Age.item() if i in age_df.encounter.values else \"Unknown\" for i in np.unique(train_eids)]\n",
        "train_age_groups = [agegroups(i) for i in train_age]\n",
        "\n",
        "print(train_encounter_embs_cls.shape)\n",
        "print(train_encounter_embs_mean_pooled.shape)\n",
        "print(train_encounter_labs.shape)\n",
        "print(train_num_notes_in_encounter.shape)\n",
        "print(train_encounter_pid.shape)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = testdata[\"labels\"]\n",
        "test_eids = testdata[\"encounter_id\"]\n",
        "test_pids = testdata[\"patient_id\"]\n",
        "test_encounter_embs_cls = np.array([np.array([test_cls_embeddings[i] for i in np.where(np.array(test_eids) == k)[0]]).mean(axis=0) for k in np.unique(test_eids)])\n",
        "test_encounter_embs_mean_pooled = np.array([np.array([test_mean_pooled_embeddings[i] for i in np.where(np.array(test_eids) == k)[0]]).mean(axis=0) for k in np.unique(test_eids)])\n",
        "test_encounter_labs = np.array([np.array([test_labels[i] for i in np.where(np.array(test_eids) == k)[0]]).mean(axis=0) for k in np.unique(test_eids)])\n",
        "test_num_notes_in_encounter = np.array([len([test_labels[i] for i in np.where(np.array(test_eids) == k)[0]]) for k in np.unique(test_eids)])\n",
        "test_encounter_pid = np.array([[test_pids[i].item() for i in np.where(np.array(test_eids) == k)[0]][0] for k in np.unique(test_eids)])\n",
        "\n",
        "intersection_test = set(test_encounter_pid.tolist()).intersection(set(dfP.DurableKey.values.tolist()))\n",
        "test_sex = [dfP[dfP[\"DurableKey\"]==i].Sex.item() if i in intersection_test else \"Unknown\" for i in test_encounter_pid]\n",
        "test_eth = [dfP[dfP[\"DurableKey\"]==i].Ethnicity.item() if i in intersection_test else \"Unknown\" for i in test_encounter_pid]\n",
        "test_age = [age_df[age_df.encounter==i].Age.item() if i in age_df.encounter.values else \"Unknown\" for i in np.unique(test_eids)]\n",
        "test_age_groups = [agegroups(i) for i in test_age]\n",
        "\n",
        "print(test_encounter_embs_cls.shape)\n",
        "print(test_encounter_embs_mean_pooled.shape)\n",
        "print(test_encounter_labs.shape)\n",
        "print(test_num_notes_in_encounter.shape)\n",
        "print(test_encounter_pid.shape)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA()\n",
        "pipe = Pipeline([('scaler', StandardScaler()), ('pca', pca)])\n",
        "\n",
        "pca_cls = pipe.fit_transform(train_encounter_embs_cls)\n",
        "pca_mean_pooled = pipe.fit_transform(train_encounter_embs_mean_pooled)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "plot_data_cls = {'PC 1': pca_cls[:, 0],\n",
        "                 'PC 2': pca_cls[:, 1], \n",
        "                 'Label': [int(i) for i in train_encounter_labs],\n",
        "                 'Num notes': train_num_notes_in_encounter,\n",
        "                  \"Sex\": train_sex,\n",
        "                 \"Ethnicity\": train_eth,\n",
        "                 \"Age\": train_age_groups,\n",
        "                 \"Prediction\": train_preds,\n",
        "                 \"Probability\": train_probs\n",
        "                 }\n",
        "\n",
        "plot_data_mean_pooled = {'PC 1': pca_mean_pooled[:, 0], \n",
        "                         'PC 2': pca_mean_pooled[:, 1], \n",
        "                         'Label': [int(i) for i in train_encounter_labs],\n",
        "                         'Num notes': train_num_notes_in_encounter,\n",
        "                          \"Sex\": train_sex,\n",
        "                         \"Ethnicity\": train_eth,\n",
        "                         \"Age\": train_age_groups,\n",
        "                         \"Prediction\": train_preds,\n",
        "                         \"Probability\": train_probs\n",
        "                         }"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.cm as cm\n",
        "\n",
        "sns.set_style(\"white\", {\"axes.edgecolor\": \".8\"})\n",
        "colormap =sns.color_palette(palette='mako_r',n_colors=20, as_cmap=True)\n",
        "\n",
        "fig = plt.figure(figsize=(9,7))\n",
        "sns.scatterplot(data=plot_data_mean_pooled, x='PC 1', y='PC 2', hue='Num notes', s=10, cmap=colormap, palette=colormap)\n",
        "\n",
        "scalarmappaple = cm.ScalarMappable(cmap=colormap)\n",
        "scalarmappaple.set_array(plot_data_mean_pooled[\"Num notes\"])\n",
        "cbar = fig.colorbar(scalarmappaple)\n",
        "cbar.set_label('# Note splits')\n",
        "plt.legend('',frameon=False)\n",
        "\n",
        "plt.savefig(\"../../output/PCA_train_mean_pooled_noteSplit_s10_woGrid.pdf\", bbox_inches=\"tight\")\n",
        "plt.savefig(\"../../output/PCA_train_mean_pooled_noteSplit_s10_woGrid.png\", bbox_inches=\"tight\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_style(\"white\", {\"axes.edgecolor\": \".8\"})\n",
        "\n",
        "c0 = sns.color_palette(palette='mako_r',n_colors=6)[0]\n",
        "c1 = sns.color_palette(palette='mako_r',n_colors=6)[4]\n",
        "\n",
        "colors=[c0,c1]\n",
        "#colors=[c1,c0]\n",
        "\n",
        "fig = plt.figure(figsize=(7,7))\n",
        "sns.scatterplot(data=plot_data_mean_pooled, x='PC 1', y='PC 2', hue='Prediction', s=10, palette=colors)\n",
        "plt.savefig(\"../../output/PCA_train_mean_pooled_prediction_s10_woGrid.pdf\", bbox_inches=\"tight\")\n",
        "plt.savefig(\"../../output/PCA_train_mean_pooled_prediction_s10_woGrid.png\", bbox_inches=\"tight\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_style(\"white\", {\"axes.edgecolor\": \".8\"})\n",
        "\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "colormap =sns.color_palette(palette='mako_r',n_colors=100, as_cmap=True)\n",
        "\n",
        "fig = plt.figure(figsize=(9,7))\n",
        "sns.scatterplot(data=plot_data_mean_pooled, x='PC 1', y='PC 2', hue='Probability', s=10, cmap=colormap, palette=colormap)\n",
        "\n",
        "scalarmappaple = cm.ScalarMappable(cmap=colormap)\n",
        "scalarmappaple.set_array(plot_data_mean_pooled[\"Probability\"])\n",
        "scalarmappaple.set_clim(vmin=0,vmax=1)\n",
        "cbar = fig.colorbar(scalarmappaple)\n",
        "cbar.set_label('Probability')\n",
        "plt.legend('',frameon=False)\n",
        "\n",
        "plt.savefig(\"../../output/PCA_train_mean_pooled_probability_s10_woGrid.pdf\", bbox_inches=\"tight\")\n",
        "plt.savefig(\"../../output/PCA_train_mean_pooled_probability_s10_woGrid.png\", bbox_inches=\"tight\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_style(\"white\", {\"axes.edgecolor\": \".8\"})\n",
        "\n",
        "c0 = sns.color_palette(palette='mako_r',n_colors=6)[0]\n",
        "c1 = sns.color_palette(palette='mako_r',n_colors=6)[4]\n",
        "c3 = \"r\"#sns.color_palette(palette='mako_r',n_colors=10)[5]\n",
        "\n",
        "colors=[c1,c0,c3]\n",
        "colors=[c0,c1,c3]\n",
        "colors=[c0,c1]\n",
        "\n",
        "fig = plt.figure(figsize=(7,7))\n",
        "sns.scatterplot(data=plot_data_mean_pooled, \n",
        "                x='PC 1', \n",
        "                y='PC 2', \n",
        "                hue='Sex', \n",
        "                s=5, \n",
        "                palette=colors, \n",
        "                hue_order=[\"Kvinde\", \"Mand\"])\n",
        "\n",
        "#plt.savefig(\"../../output/PCA_train_mean_pooled_sex_s5_wo_unknown_woGrid.pdf\", bbox_inches=\"tight\")\n",
        "#plt.savefig(\"../../output/PCA_train_mean_pooled_sex_s5_wo_unknown_woGrid.png\", bbox_inches=\"tight\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# loading dataframe with diagnoses for encounters and getting the action diagnosis SKS (ICD-10) code\n",
        "diagnosis_df = pd.read_parquet(\"../../data/acuteReadmission/afregningsdiagnose-copy.parquet\")\n",
        "\n",
        "def sort_SKS_hierarchy(SKSCode):\n",
        "    \"\"\"\n",
        "    Function to keep DF20 as the most important for action diagnosis.\n",
        "    \"\"\"\n",
        "    if SKSCode.startswith('DF'):\n",
        "        if SKSCode == 'DF20':\n",
        "            return '0' + SKSCode\n",
        "        return SKSCode\n",
        "    return 'Z' + SKSCode  # Place non-'DF' codes at the end\n",
        "\n",
        "def get_action_diagnosis(afregnings):\n",
        "    \"\"\"\n",
        "    Function to get information about the action diagnosis or\n",
        "    main diagnosis related to the admission.\n",
        "    Parameters\n",
        "    ------------\n",
        "    - afregnings. Afregningsdiagnose dataframe.\n",
        "    \"\"\"\n",
        "    afregnings_action = afregnings[\n",
        "        ['PatientDurableKey',\n",
        "        'EncounterKey',\n",
        "        'SKSCode',\n",
        "        'IsActionDiagnosis']\n",
        "    ]\n",
        "    # We only want one main for encounter since we merge to encounters\n",
        "    afregnings_action = afregnings_action[afregnings_action.IsActionDiagnosis == 1]\n",
        "    afregnings_action.drop(columns='IsActionDiagnosis', inplace = True)\n",
        "    # We need to connect to encounters table --> we need some encounterKey\n",
        "    afregnings_action = afregnings_action[afregnings_action.EncounterKey != -1]\n",
        "    # Create a temporary sorting column based on hierarchy\n",
        "    afregnings_action['SortingColumn'] = afregnings_action['SKSCode'].apply(sort_SKS_hierarchy)\n",
        "    # Group by EncounterKey, sort, and select the first row in each group\n",
        "    afregnings_action = afregnings_action.sort_values(\n",
        "        by=['EncounterKey', 'SortingColumn']\n",
        "        ).groupby('EncounterKey').head(1)\n",
        "    \n",
        "    afregnings_action = afregnings_action.drop(columns='SortingColumn')\n",
        "    # Reset the index\n",
        "    afregnings_action = afregnings_action.reset_index(drop=True)\n",
        "    return afregnings_action\n",
        "\n",
        "afregnings_action = get_action_diagnosis(diagnosis_df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "intersection_diagnosis_train = set(train_eids.tolist()).intersection(set(afregnings_action.EncounterKey.values.tolist()))\n",
        "\n",
        "train_diagnosis = np.array([afregnings_action[afregnings_action.EncounterKey==i].SKSCode.item() if i in intersection_diagnosis_train else \"unknown\" for i in np.unique(train_eids)])\n",
        "print(train_diagnosis.shape)\n",
        "np.save(\"train_diagnosis.npy\", train_diagnosis)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Possibly load the result\n",
        "train_diagnosis = np.load(\"train_diagnosis.npy\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def skscode_to_diagnosis(sks):\n",
        "    if sks.startswith(\"DF20\"):\n",
        "        return \"Schizophrenia\"\n",
        "    elif sks.startswith(\"DF2\"):\n",
        "        return \"Other psychosis\"\n",
        "    elif sks.startswith(\"DF30\") or sks.startswith(\"DF31\"):\n",
        "        return \"Bipolar/manic\"\n",
        "    elif sks.startswith(\"DF32\") or sks.startswith(\"DF33\"):\n",
        "        return \"Depression\"\n",
        "    elif sks.startswith(\"DF40\") or sks.startswith(\"DF41\") or sks.startswith(\"DF42\"):\n",
        "        return \"Anxiety/OCD\"\n",
        "    elif sks.startswith(\"DF6\"):\n",
        "        return \"Personality disorder\"\n",
        "    elif sks.startswith(\"DF1\"):\n",
        "        return \"SUD\"\n",
        "    else:\n",
        "        return \"Other\"\n",
        "\n",
        "train_diagnosis_simple = np.array([sks[1:3] if sks.startswith(\"DF\") else \"Other\" for sks in train_diagnosis])\n",
        "\n",
        "train_diagnosis_specific = np.array([skscode_to_diagnosis(sks) for sks in train_diagnosis])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "plot_data_cls = {'PC 1': pca_cls[:, 0],\n",
        "                 'PC 2': pca_cls[:, 1], \n",
        "                 'Label': [int(i) for i in train_encounter_labs],\n",
        "                 'Num notes': train_num_notes_in_encounter,\n",
        "                  \"Sex\": train_sex,\n",
        "                 \"Ethnicity\": train_eth,\n",
        "                 \"Age\": train_age_groups,\n",
        "                 \"Prediction\": train_preds,\n",
        "                 \"Probability\": train_probs,\n",
        "                 \"Diagnosis\": train_diagnosis_simple,\n",
        "                 \"Diagnosis_specific\": train_diagnosis_specific\n",
        "                 }\n",
        "\n",
        "plot_data_mean_pooled = {'PC 1': pca_mean_pooled[:, 0], \n",
        "                         'PC 2': pca_mean_pooled[:, 1],\n",
        "                         'Label': [int(i) for i in train_encounter_labs],\n",
        "                         'Num notes': train_num_notes_in_encounter,\n",
        "                          \"Sex\": train_sex,\n",
        "                         \"Ethnicity\": train_eth,\n",
        "                         \"Age\": train_age_groups,\n",
        "                         \"Prediction\": train_preds,\n",
        "                         \"Probability\": train_probs,\n",
        "                         \"Diagnosis\": train_diagnosis_simple,\n",
        "                         \"Diagnosis_specific\": train_diagnosis_specific\n",
        "                         }"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_style(\"white\", {\"axes.edgecolor\": \".8\"})\n",
        "\n",
        "colors = sns.color_palette(palette='colorblind',n_colors=len(np.unique(train_diagnosis_specific))+3)\n",
        "colors = [colors[0]]+colors[2:5]+colors[7:10]#+colors[13:]\n",
        "#print(colors)\n",
        "\n",
        "fig = plt.figure(figsize=(7,7))\n",
        "sns.scatterplot(data=plot_data_mean_pooled, \n",
        "                x='PC 1', \n",
        "                y='PC 2', \n",
        "                hue='Diagnosis_specific', \n",
        "                s=10, \n",
        "                palette=colors+[\"black\"], \n",
        "                #style=\"Diagnosis_specific\",\n",
        "                hue_order= ['Anxiety/OCD', \n",
        "                            'Personality disorder', \n",
        "                            'Bipolar/manic', \n",
        "                            'Depression', \n",
        "                            'Other psychosis', \n",
        "                            'SUD', \n",
        "                            'Schizophrenia' \n",
        "                            ][::-1]+[\"Other\"]\n",
        "               )\n",
        "plt.legend(loc=\"upper right\", ncol=1, prop={'size': 9});\n",
        "plt.savefig(\"../../output/PCA_train_mean_pooled_diagnosis_specific_fig77_s10_colorblind_noGrid.pdf\", bbox_inches=\"tight\")\n",
        "plt.savefig(\"../../output/PCA_train_mean_pooled_diagnosis_specific_fig77_s10_colorblind_noGrid.png\", bbox_inches=\"tight\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_style(\"white\", {\"axes.edgecolor\": \".8\"})\n",
        "\n",
        "colors = sns.color_palette(palette='colorblind',n_colors=len(np.unique(train_diagnosis_simple))-1)\n",
        "\n",
        "fig = plt.figure(figsize=(7,7))\n",
        "sns.scatterplot(data=plot_data_mean_pooled, \n",
        "                x='PC 1', \n",
        "                y='PC 2', \n",
        "                hue='Diagnosis', \n",
        "                s=10, \n",
        "                palette=colors+[\"black\"], \n",
        "                #style=\"Diagnosis\",\n",
        "                hue_order= [\"F0\", \"F1\", \"F2\", \"F3\", \"F4\", \"F5\", \"F6\", \"F7\", \"F8\", \"F9\", \"Other\"]\n",
        "               )\n",
        "plt.legend(loc=\"upper right\", ncol=2);\n",
        "\n",
        "plt.savefig(\"../../output/PCA_train_mean_pooled_Fdiagnosis_fig77_s10_colorblind_noGrid.pdf\", bbox_inches=\"tight\")\n",
        "plt.savefig(\"../../output/PCA_train_mean_pooled_Fdiagnosis_fig77_s10_colorblind_noGrid.png\", bbox_inches=\"tight\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, roc_auc_score, matthews_corrcoef,  precision_recall_curve, roc_curve, auc, precision_score, recall_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "def compute_specificity(targets, preds):\n",
        "    if len(np.unique(targets))>2:\n",
        "        return None\n",
        "    else:\n",
        "        return recall_score(targets, preds, pos_label=0, average=\"binary\")\n",
        "\n",
        "def compute_auroc(targets,preds):\n",
        "    if len(np.unique(targets))>2:\n",
        "        auc_score = roc_auc_score(\n",
        "                            targets,\n",
        "                            preds,\n",
        "                            multi_class=\"ovr\",\n",
        "                            average=\"macro\",\n",
        "                        )\n",
        "    else:\n",
        "        fpr, tpr, thresholds = roc_curve(targets,preds[:,1])\n",
        "        auc_score = auc(fpr, tpr)\n",
        "    return auc_score\n",
        "\n",
        "def compute_auprc(targets,preds):\n",
        "    precision, recall, _ = precision_recall_curve(targets, preds)\n",
        "    area = auc(recall,precision)\n",
        "    return area\n",
        "\n",
        "#def compute_f1_neg(targets,preds):\n",
        "#    return f1_score(targets,preds,pos_label=0)\n",
        "\n",
        "def compute_f1_weighted(targets,preds):\n",
        "    return f1_score(targets,preds,average=\"weighted\")\n",
        "\n",
        "def compute_f1(targets,preds):\n",
        "    if len(np.unique(targets))>2:\n",
        "        return f1_score(targets, preds, average=\"weighted\")\n",
        "    else:\n",
        "        return f1_score(targets, preds, average=\"binary\")\n",
        "        \n",
        "def compute_precision(targets,preds):\n",
        "    if len(np.unique(targets))>2:\n",
        "        return precision_score(targets, preds, average=\"weighted\")\n",
        "    else:\n",
        "        return precision_score(targets, preds, average=\"binary\")\n",
        "\n",
        "def compute_recall(targets,preds):\n",
        "    if len(np.unique(targets))>2:\n",
        "        return recall_score(targets, preds, average=\"weighted\")\n",
        "    else:\n",
        "        return recall_score(targets, preds, average=\"binary\")\n",
        "\n",
        "\n",
        "def run_eval(probs,preds,targets):\n",
        "    \n",
        "    metrics_results_list = [compute_auroc(targets, probs),\n",
        "                          matthews_corrcoef(targets, preds),\n",
        "                          compute_f1_weighted(targets, preds),\n",
        "                          compute_f1(targets, preds),\n",
        "                          compute_precision(targets, preds),\n",
        "                          compute_recall(targets, preds)\n",
        "                           ]\n",
        "    \n",
        "    index_ = [\"AUC\",\"MCC\",\"F1 AVG\", \"F1\",\"precision\",\"recall\"]\n",
        "    \n",
        "    return metrics_results_list, index_\n",
        "    \n",
        "\n",
        "\n",
        "def run_knn(train_embs, train_labs, test_embs, test_labs, k):\n",
        "    \n",
        "    KNN = Pipeline(\n",
        "    steps=[(\"scaler\", StandardScaler()), (\"knn\", KNeighborsClassifier(n_neighbors=k, n_jobs=12))])\n",
        "    \n",
        "    KNN.fit(train_embs, train_labs)\n",
        "    \n",
        "    test_probs = KNN.predict_proba(test_embs)\n",
        "    \n",
        "    train_probs = KNN.predict_proba(train_embs)\n",
        "    \n",
        "    return (test_probs, train_probs)\n",
        "\n",
        "def eval_and_save(test_probs, train_probs, test_labs, train_labs, k, embs_type, task):\n",
        "    \n",
        "    test_preds = np.argmax(test_probs,axis=1)\n",
        "    train_preds = np.argmax(train_probs,axis=1)\n",
        "   \n",
        "    test_metrics, _ = run_eval(test_probs, test_preds, test_labs)\n",
        "    ##print(test_metrics)\n",
        "    \n",
        "    train_metrics, index_ = run_eval(train_probs, train_preds, train_labs)\n",
        "    #print(train_metrics)\n",
        "    \n",
        "    metric_results = pd.DataFrame(list(zip(train_metrics,test_metrics)), \n",
        "                                  columns = [\"Train\", \"Test\"],\n",
        "                                  index = index_)\n",
        "    \n",
        "    metric_results.to_csv(\"../../output/KNN_{}_{}_{}.csv\".format(k,embs_type,task))\n",
        "    \n",
        "    return metric_results"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_mean_pooled = {'Embs': train_encounter_embs_mean_pooled,\n",
        "                         'Label': [int(i) for i in train_encounter_labs],\n",
        "                         'Num notes': train_num_notes_in_encounter,\n",
        "                          \"Sex\": train_sex,\n",
        "                         \"Ethnicity\": train_eth,\n",
        "                         \"Age\": train_age_groups,\n",
        "                         \"Prediction\": train_preds,\n",
        "                         \"Probability\": train_probs,\n",
        "                         \"Diagnosis\": train_diagnosis_simple,\n",
        "                         \"Diagnosis_specific\": train_diagnosis_specific\n",
        "                         }\n",
        "\n",
        "intersection_diagnosis_test = set(test_eids.tolist()).intersection(set(afregnings_action.EncounterKey.values.tolist()))\n",
        "\n",
        "test_diagnosis = np.array([afregnings_action[afregnings_action.EncounterKey==i].SKSCode.item() if i in intersection_diagnosis_test else \"unknown\" for i in np.unique(test_eids)])\n",
        "print(test_diagnosis.shape)\n",
        "\n",
        "test_diagnosis_simple = np.array([sks[1:3] if sks.startswith(\"DF\") else \"Other\" for sks in test_diagnosis])\n",
        "test_diagnosis_specific = np.array([skscode_to_diagnosis(sks) for sks in test_diagnosis])\n",
        "\n",
        "test_data_mean_pooled = {'Embs': test_encounter_embs_mean_pooled,\n",
        "                         'Label': [int(i) for i in test_encounter_labs],\n",
        "                         'Num notes': test_num_notes_in_encounter,\n",
        "                          \"Sex\": test_sex,\n",
        "                         \"Ethnicity\": test_eth,\n",
        "                         \"Age\": test_age_groups,\n",
        "                         \"Diagnosis\": test_diagnosis_simple,\n",
        "                         \"Diagnosis_specific\": test_diagnosis_specific\n",
        "                         }"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_embs = train_encounter_embs_mean_pooled\n",
        "train_labs = train_encounter_labs\n",
        "test_embs = test_encounter_embs_mean_pooled\n",
        "test_labs = test_encounter_labs\n",
        "\n",
        "embs_type = \"mean_pooled\"\n",
        "task = \"finetuned_psyroberta_label\"\n",
        "\n",
        "for k in [5,10,50]:\n",
        "    test_probs, train_probs = run_knn(train_embs, train_labs, test_embs, test_labs, k)\n",
        "    eval_and_save(test_probs, train_probs, test_labs, train_labs, k, embs_type, task)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_embs = train_encounter_embs_mean_pooled\n",
        "id2index = {k:v for v, k in enumerate(np.unique(train_diagnosis_simple))}\n",
        "train_labs = [id2index[i] for i in train_diagnosis_simple]\n",
        "\n",
        "test_embs = test_encounter_embs_mean_pooled\n",
        "test_labs = [id2index[i] for i in test_diagnosis_simple]\n",
        "\n",
        "embs_type = \"mean_pooled\"\n",
        "task = \"finetuned_psyroberta_diagnosis\"\n",
        "\n",
        "for k in [5,10,50]:\n",
        "    test_probs, train_probs = run_knn(train_embs, train_labs, test_embs, test_labs, k)\n",
        "    eval_and_save(test_probs, train_probs, test_labs, train_labs, k, embs_type, task)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_embs = train_encounter_embs_mean_pooled\n",
        "id2index = {k:v for v, k in enumerate(np.unique(train_eth))}\n",
        "train_labs = [id2index[i] for i in train_eth]\n",
        "\n",
        "test_embs = test_encounter_embs_mean_pooled\n",
        "test_labs = [id2index[i] for i in test_eth]\n",
        "\n",
        "embs_type = \"mean_pooled\"\n",
        "task = \"finetuned_psyroberta_ethnicity\"\n",
        "\n",
        "for k in [5,10,50]:\n",
        "    test_probs, train_probs = run_knn(train_embs, train_labs, test_embs, test_labs, k)\n",
        "    eval_and_save(test_probs, train_probs, test_labs, train_labs, k, embs_type, task)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_embs = train_encounter_embs_mean_pooled\n",
        "id2index = {k:v for v, k in enumerate(np.unique(train_diagnosis_specific))}\n",
        "train_labs = [id2index[i] for i in train_diagnosis_specific]\n",
        "\n",
        "test_embs = test_encounter_embs_mean_pooled\n",
        "test_labs = [id2index[i] for i in test_diagnosis_specific]\n",
        "\n",
        "\n",
        "embs_type = \"mean_pooled\"\n",
        "task = \"finetuned_psyroberta_diagnosis_specific\"\n",
        "\n",
        "for k in [5,10,50]:\n",
        "    test_probs, train_probs = run_knn(train_embs, train_labs, test_embs, test_labs, k)\n",
        "    eval_and_save(test_probs, train_probs, test_labs, train_labs, k, embs_type, task)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_indels = [i for i,j in enumerate(train_age_groups) if j != \"Unknown\"]\n",
        "train_age_groups_ = np.array(train_age_groups)[np.array(train_indels)]\n",
        "\n",
        "train_embs = train_encounter_embs_mean_pooled[np.array(train_indels)]\n",
        "id2index = {k:v for v, k in enumerate(np.unique(train_age_groups_))}\n",
        "train_labs = [id2index[i] for i in train_age_groups_]\n",
        "\n",
        "\n",
        "test_indels = [i for i,j in enumerate(test_age_groups) if j != \"Unknown\"]\n",
        "test_age_groups_ = np.array(test_age_groups)[np.array(test_indels)]\n",
        "\n",
        "test_embs = test_encounter_embs_mean_pooled[np.array(test_indels)]\n",
        "test_labs = [id2index[i] for i in test_age_groups_]\n",
        "\n",
        "embs_type = \"mean_pooled\"\n",
        "task = \"finetuned_psyroberta_age\"\n",
        "\n",
        "for k in [5,10,50]:\n",
        "    test_probs, train_probs = run_knn(train_embs, train_labs, test_embs, test_labs, k)\n",
        "    eval_and_save(test_probs, train_probs, test_labs, train_labs, k, embs_type, task)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_indels = [i for i,j in enumerate(train_sex) if j != \"Unknown\"]\n",
        "train_sex_ = np.array(train_sex)[np.array(train_indels)]\n",
        "\n",
        "train_embs = train_encounter_embs_mean_pooled[np.array(train_indels)]\n",
        "id2index = {k:v for v, k in enumerate(np.unique(train_sex_))}\n",
        "train_labs = [id2index[i] for i in train_sex_]\n",
        "\n",
        "test_indels = [i for i,j in enumerate(test_sex) if j != \"Unknown\"]\n",
        "test_sex_ = np.array(test_sex)[np.array(test_indels)]\n",
        "test_embs = test_encounter_embs_mean_pooled[np.array(test_indels)]\n",
        "test_labs = [id2index[i] for i in test_sex_]\n",
        "\n",
        "embs_type = \"mean_pooled\"\n",
        "task = \"finetuned_psyroberta_sex_wo_unknown\"\n",
        "\n",
        "for k in [5,10,50]:\n",
        "    test_probs, train_probs = run_knn(train_embs, train_labs, test_embs, test_labs, k)\n",
        "    eval_and_save(test_probs, train_probs, test_labs, train_labs, k, embs_type, task)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}